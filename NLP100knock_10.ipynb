{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第10章"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 90. データの準備\n",
    "機械翻訳のデータセットをダウンロードせよ．訓練データ，開発データ，評価データを整形し，必要に応じてトークン化などの前処理を行うこと．ただし，この段階ではトークンの単位として形態素（日本語）および単語（英語）を採用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-06-17 10:09:16--  http://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz\n",
      "Resolving www.phontron.com (www.phontron.com)... 208.113.196.149\n",
      "Connecting to www.phontron.com (www.phontron.com)|208.113.196.149|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 99246893 (95M) [application/gzip]\n",
      "Saving to: ‘kftt-data-1.0.tar.gz’\n",
      "\n",
      "kftt-data-1.0.tar.g 100%[===================>]  94.65M  16.8MB/s    in 7.6s    \n",
      "\n",
      "2022-06-17 10:09:24 (12.4 MB/s) - ‘kftt-data-1.0.tar.gz’ saved [99246893/99246893]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kftt-data-1.0/\n",
      "kftt-data-1.0/data/\n",
      "kftt-data-1.0/data/orig/\n",
      "kftt-data-1.0/data/orig/kyoto-tune.en\n",
      "kftt-data-1.0/data/orig/kyoto-dev.ja\n",
      "kftt-data-1.0/data/orig/kyoto-dev.en\n",
      "kftt-data-1.0/data/orig/kyoto-train.en\n",
      "kftt-data-1.0/data/orig/kyoto-tune.ja\n",
      "kftt-data-1.0/data/orig/kyoto-train.ja\n",
      "kftt-data-1.0/data/orig/kyoto-test.ja\n",
      "kftt-data-1.0/data/orig/kyoto-test.en\n",
      "kftt-data-1.0/data/tok/\n",
      "kftt-data-1.0/data/tok/kyoto-tune.en\n",
      "kftt-data-1.0/data/tok/kyoto-dev.ja\n",
      "kftt-data-1.0/data/tok/kyoto-train.cln.en\n",
      "kftt-data-1.0/data/tok/kyoto-dev.en\n",
      "kftt-data-1.0/data/tok/kyoto-train.en\n",
      "kftt-data-1.0/data/tok/kyoto-tune.ja\n",
      "kftt-data-1.0/data/tok/kyoto-train.cln.ja\n",
      "kftt-data-1.0/data/tok/kyoto-train.ja\n",
      "kftt-data-1.0/data/tok/kyoto-test.ja\n",
      "kftt-data-1.0/data/tok/kyoto-test.en\n",
      "kftt-data-1.0/README.txt\n"
     ]
    }
   ],
   "source": [
    "!tar zxvf kftt-data-1.0.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: missing operand\n",
      "Try 'mkdir --help' for more information.\n"
     ]
    }
   ],
   "source": [
    "!mkdir \n",
    "!mkdir kyoto\n",
    "!cp -r kftt-data-1.0/data/tok/kyoto* ./kyoto\n",
    "!rm -rf kyoto/kyoto*cln* kyoto/kyoto*tune* \n",
    "!mkdir kyoto/kyoto-bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 10 kyoto/kyoto-test.ja > test10.ja \n",
    "!head -n 10 kyoto/kyoto-test.en > test10.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in ['kftt-data-1.0/data/tok/kyoto-train.ja', 'kftt-data-1.0/data/tok/kyoto-train.en',\n",
    "             'kftt-data-1.0/data/tok/kyoto-dev.ja', 'kftt-data-1.0/data/tok/kyoto-dev.en',\n",
    "             'kftt-data-1.0/data/tok/kyoto-test.ja', 'kftt-data-1.0/data/tok/kyoto-test.en',\n",
    "            ]:\n",
    "    with open(file) as f:\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 91. 機械翻訳モデルの訓練\n",
    "90で準備したデータを用いて，ニューラル機械翻訳のモデルを学習せよ（ニューラルネットワークのモデルはTransformerやLSTMなど適当に選んでよい）．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fairseq：フェイスブックが開発元の機械翻訳用のフレームワーク, pytorchがベースとなっている。\n",
    "[fairseq](https://qiita.com/keita_M/items/3a7401ef48df3ec92b6f)使い方\n",
    "\n",
    "* fairseq-preprocess: データセットの前処理\n",
    "* fairseq-train: モデルの学習\n",
    "* fairseq-generate: 与えられたテストセットに対して学習済みモデルから出力を生成\n",
    "* fiarseq-interactive: インタラクティブに学習済みモデルから出力を生成\n",
    "* fairseq-score: BLEUスコアを計算 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[fairseq-preprocessドキュメント](https://fairseq.readthedocs.io/en/latest/command_line_tools.html?highlight=fairseq-preprocess%20#fairseq-preprocess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|オプション|説明|\n",
    "|:--:|:--|\n",
    "|train/valid/testpref|各データのパスプレフィックス|\n",
    "|source/target-lang|ソース・ターゲットのデータの拡張子|\n",
    "|destdir|バイナリ化したデータの保存先ディレクトリ|\n",
    "|tokenize|トークナイザー|\n",
    "|bpe|BPEを行うトークナイザー|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BPEとはByte Pair Encodingの頭文字であり、文書における低頻度の単語をさらに分割することで、低頻度の単語もうまく扱えるようにする手法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-06 09:28:02 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='kyoto/kyoto-bin', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, simul_type=None, source_lang='ja', srcdict=None, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref='kyoto/kyoto-test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer='space', tpu=False, trainpref='kyoto/kyoto-train', use_plasma_view=False, user_dir=None, validpref='kyoto/kyoto-dev', wandb_project=None, workers=1)\n",
      "2022-08-06 09:29:19 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 146832 types\n",
      "2022-08-06 09:30:45 | INFO | fairseq_cli.preprocess | [ja] kyoto/kyoto-train.ja: 440288 sents, 12359715 tokens, 0.0% replaced (by <unk>)\n",
      "2022-08-06 09:30:45 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 146832 types\n",
      "2022-08-06 09:30:46 | INFO | fairseq_cli.preprocess | [ja] kyoto/kyoto-dev.ja: 1166 sents, 28010 tokens, 0.518% replaced (by <unk>)\n",
      "2022-08-06 09:30:46 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 146832 types\n",
      "2022-08-06 09:30:46 | INFO | fairseq_cli.preprocess | [ja] kyoto/kyoto-test.ja: 1160 sents, 29638 tokens, 0.55% replaced (by <unk>)\n",
      "2022-08-06 09:30:46 | INFO | fairseq_cli.preprocess | [en] Dictionary: 221864 types\n",
      "2022-08-06 09:31:48 | INFO | fairseq_cli.preprocess | [en] kyoto/kyoto-train.en: 440288 sents, 11981667 tokens, 0.0% replaced (by <unk>)\n",
      "2022-08-06 09:31:48 | INFO | fairseq_cli.preprocess | [en] Dictionary: 221864 types\n",
      "2022-08-06 09:31:48 | INFO | fairseq_cli.preprocess | [en] kyoto/kyoto-dev.en: 1166 sents, 25475 tokens, 1.79% replaced (by <unk>)\n",
      "2022-08-06 09:31:48 | INFO | fairseq_cli.preprocess | [en] Dictionary: 221864 types\n",
      "2022-08-06 09:31:48 | INFO | fairseq_cli.preprocess | [en] kyoto/kyoto-test.en: 1160 sents, 27894 tokens, 1.55% replaced (by <unk>)\n",
      "2022-08-06 09:31:48 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to kyoto/kyoto-bin\n"
     ]
    }
   ],
   "source": [
    "!fairseq-preprocess --source-lang ja --target-lang en \\\n",
    "   --trainpref kyoto/kyoto-train \\\n",
    "   --validpref kyoto/kyoto-dev \\\n",
    "   --testpref kyoto/kyoto-test \\\n",
    "   --tokenizer space \\\n",
    "   --destdir kyoto/kyoto-bin "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[tasksオプション](https://fairseq.readthedocs.io/en/v0.10.2/tasks.html)\n",
    "\n",
    "\n",
    "|オプション|説明|\n",
    "|:--|:--|\n",
    "|--arch|使用するモデルのアーキテクチャを指定|\n",
    "|--task|defaultでtranslation|\n",
    "|--restore-file|事前学習済みのものをfinetuneするならファイルを指定|\n",
    "|--lr|learning rateの初期値, default=0.25となってるから指定することを推奨|\n",
    "|--warmup-updates|このパラメータの回数分ミニバッチを処理した段階でのlrが最大値になる|\n",
    "|--max-epoch|epoch数の指定, 指定しないと永遠に学習を続けるので適当に値を入れておくといい|\n",
    "|--eval-bleu|BLEUで評価|\n",
    "|--eval-bleu-detok|detokenizeする分割トークン|\n",
    "|--eval-bleu-detok-args|args for building the tokenizer, if needed|\n",
    "|--eval-bleu-remove-bpe|remove BPE before computing BLEU|\n",
    "|--best-checkpoint-metric|入出力で使用するembeddingを共有する|\n",
    "|--maximize-best-checkpoint-metric|最良チェックポイントを保存するために、最大の行列値を選択|\n",
    "|--fp16|半浮動小数点を使用し、データ量を削減・スループットの向上|\n",
    "|--no-epoch-checkpoints|最後と最良のcheckpointだけを保存|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 fairseq-train kyoto/kyoto-bin \\\n",
    "    --save-dir checkpoints/kyoto \\\n",
    "    --tensorboard-logdir tensorboard/kyoto \\\n",
    "    --arch transformer --task translation \\\n",
    "\t--share-decoder-input-output-embed \\\n",
    " \t--optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "\t--lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "\t--criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "\t--dropout 0.3 --weight-decay 0.0001 \\\n",
    "\t--max-tokens 4096 \\\n",
    "    --eval-bleu \\\n",
    "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
    "    --eval-bleu-detok space \\\n",
    "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
    "\t--fp16 \\\n",
    "    --max-epoch 10\n",
    "    --no-epoch-checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 92.機械翻訳モデルの適用\n",
    "91で学習したニューラル機械翻訳モデルを用い，与えられた（任意の）日本語の文を英語に翻訳するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fairseq-interactiveの出力\n",
    "```\n",
    "S-0\t曹洞 宗 の 開祖 。\n",
    "W-0\t0.618\tseconds\n",
    "H-0\t-1.0414628982543945\tHe was the founder of the Rinzai Sect .\n",
    "D-0\t-1.0414628982543945\tHe was the founder of the Rinzai Sect .\n",
    "P-0\t-3.4044 -0.5500 -0.4172 -0.7782 -0.0934 -0.3429 -2.5765 -0.9562 -1.2363 -0.0595\n",
    "```\n",
    "\n",
    "* S: 入力文\n",
    "* W: 推論時間\n",
    "* H: トークン化された推論結果\n",
    "* D: デトークナイズされた推論結果\n",
    "* P: トークン毎の確率\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-06 12:13:50 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-06 12:13:50 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-06 12:13:50 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-06 12:13:50 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto/checkpoint_best.pt\n",
      "2022-08-06 12:14:00 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-06 12:14:00 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-06 12:20:05 | INFO | fairseq_cli.interactive | Total time: 375.486 seconds; translation time: 359.346\n"
     ]
    }
   ],
   "source": [
    "!fairseq-interactive kyoto/kyoto-bin \\\n",
    "    --path checkpoints/kyoto/checkpoint_best.pt  \\\n",
    "    --input kyoto/kyoto-test.ja \\\n",
    "    --task translation \\\n",
    "    | grep '^H' | cut -f3 > 92.out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 93. BLEUスコアの計測\n",
    "91で学習したニューラル機械翻訳モデルの品質を調べるため，評価データにおけるBLEUスコアを測定せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(ignore_case=False, order=4, ref='kyoto/kyoto-test.en', sacrebleu=False, sentence_bleu=False, sys='92.out')\n",
      "BLEU4 = 6.53, 34.0/9.6/3.9/2.0 (BP=0.921, ratio=0.924, syslen=24695, reflen=26734)\n"
     ]
    }
   ],
   "source": [
    "!fairseq-score --sys 92.out --ref kyoto/kyoto-test.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 94. ビーム探索\n",
    "91で学習したニューラル機械翻訳モデルで翻訳文をデコードする際に，ビーム探索を導入せよ．ビーム幅を1から100くらいまで適当に変化させながら，開発セット上のBLEUスコアの変化をプロットせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir 94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-06 13:02:14 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 1, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-06 13:02:15 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-06 13:02:15 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-06 13:02:15 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto/checkpoint_best.pt\n",
      "2022-08-06 13:02:24 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-06 13:02:24 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-06 13:06:18 | INFO | fairseq_cli.interactive | Total time: 243.894 seconds; translation time: 227.677\n",
      "2022-08-06 13:06:23 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 2, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-06 13:06:23 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-06 13:06:23 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-06 13:06:23 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto/checkpoint_best.pt\n",
      "2022-08-06 13:06:33 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-06 13:06:33 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-06 13:11:05 | INFO | fairseq_cli.interactive | Total time: 282.605 seconds; translation time: 266.606\n",
      "2022-08-06 13:11:10 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 3, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-06 13:11:11 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-06 13:11:11 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-06 13:11:11 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto/checkpoint_best.pt\n",
      "2022-08-06 13:11:20 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-06 13:11:20 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-06 13:16:17 | INFO | fairseq_cli.interactive | Total time: 307.124 seconds; translation time: 291.090\n",
      "2022-08-06 13:16:22 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 4, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-06 13:16:22 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-06 13:16:22 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-06 13:16:22 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto/checkpoint_best.pt\n",
      "2022-08-06 13:16:32 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-06 13:16:32 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-06 13:22:10 | INFO | fairseq_cli.interactive | Total time: 348.618 seconds; translation time: 332.734\n",
      "2022-08-06 13:22:15 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-06 13:22:16 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-06 13:22:16 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-06 13:22:16 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto/checkpoint_best.pt\n",
      "2022-08-06 13:22:25 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-06 13:22:25 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-06 13:28:41 | INFO | fairseq_cli.interactive | Total time: 385.578 seconds; translation time: 369.635\n",
      "2022-08-06 13:28:45 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 6, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-06 13:28:46 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-06 13:28:46 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-06 13:28:46 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto/checkpoint_best.pt\n",
      "2022-08-06 13:28:55 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-06 13:28:55 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-06 13:35:30 | INFO | fairseq_cli.interactive | Total time: 405.098 seconds; translation time: 390.056\n",
      "2022-08-06 13:35:35 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 7, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-06 13:35:36 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-06 13:35:36 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-06 13:35:36 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto/checkpoint_best.pt\n",
      "2022-08-06 13:35:45 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-06 13:35:45 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-06 13:43:04 | INFO | fairseq_cli.interactive | Total time: 449.432 seconds; translation time: 433.659\n",
      "2022-08-06 13:43:09 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 8, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-06 13:43:10 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-06 13:43:10 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-06 13:43:10 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto/checkpoint_best.pt\n",
      "2022-08-06 13:43:19 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-06 13:43:19 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-06 13:52:36 | INFO | fairseq_cli.interactive | Total time: 567.113 seconds; translation time: 551.160\n",
      "2022-08-06 13:52:41 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 9, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-06 13:52:41 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-06 13:52:41 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-06 13:52:41 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto/checkpoint_best.pt\n",
      "2022-08-06 13:52:51 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-06 13:52:51 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-06 14:02:59 | INFO | fairseq_cli.interactive | Total time: 617.974 seconds; translation time: 601.464\n",
      "2022-08-06 14:03:03 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 10, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-06 14:03:04 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-06 14:03:04 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-06 14:03:04 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto/checkpoint_best.pt\n",
      "2022-08-06 14:03:13 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-06 14:03:13 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-06 14:14:06 | INFO | fairseq_cli.interactive | Total time: 663.161 seconds; translation time: 647.264\n",
      "2022-08-06 14:14:11 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 11, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-06 14:14:12 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-06 14:14:12 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-06 14:14:12 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto/checkpoint_best.pt\n",
      "2022-08-06 14:14:21 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-06 14:14:21 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-06 14:25:54 | INFO | fairseq_cli.interactive | Total time: 702.990 seconds; translation time: 686.943\n",
      "2022-08-06 14:25:59 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 12, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-06 14:25:59 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-06 14:25:59 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-06 14:25:59 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto/checkpoint_best.pt\n",
      "2022-08-06 14:26:09 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-06 14:26:09 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-06 14:38:40 | INFO | fairseq_cli.interactive | Total time: 761.532 seconds; translation time: 745.599\n",
      "2022-08-06 14:38:45 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 13, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-06 14:38:45 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-06 14:38:45 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-06 14:38:45 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto/checkpoint_best.pt\n",
      "2022-08-06 14:38:55 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-06 14:38:55 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-06 14:52:15 | INFO | fairseq_cli.interactive | Total time: 810.539 seconds; translation time: 794.527\n",
      "2022-08-06 14:52:20 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 14, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-06 14:52:21 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-06 14:52:21 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-06 14:52:21 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto/checkpoint_best.pt\n",
      "2022-08-06 14:52:31 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-06 14:52:31 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-06 15:06:38 | INFO | fairseq_cli.interactive | Total time: 858.225 seconds; translation time: 841.651\n",
      "2022-08-06 15:06:43 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 15, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-06 15:06:44 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-06 15:06:44 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-06 15:06:44 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto/checkpoint_best.pt\n",
      "2022-08-06 15:06:53 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-06 15:06:53 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-06 15:21:56 | INFO | fairseq_cli.interactive | Total time: 912.960 seconds; translation time: 896.327\n",
      "2022-08-06 15:22:01 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 16, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-06 15:22:01 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-06 15:22:01 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-06 15:22:01 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto/checkpoint_best.pt\n",
      "2022-08-06 15:22:11 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-06 15:22:11 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-06 15:38:05 | INFO | fairseq_cli.interactive | Total time: 964.372 seconds; translation time: 947.995\n",
      "2022-08-06 15:38:10 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 17, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-06 15:38:10 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-06 15:38:10 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-06 15:38:10 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto/checkpoint_best.pt\n",
      "2022-08-06 15:38:20 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-06 15:38:20 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-06 15:55:00 | INFO | fairseq_cli.interactive | Total time: 1010.606 seconds; translation time: 994.119\n",
      "2022-08-06 15:55:05 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 18, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-06 15:55:05 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-06 15:55:05 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-06 15:55:05 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto/checkpoint_best.pt\n",
      "2022-08-06 15:55:15 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-06 15:55:15 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-06 16:12:35 | INFO | fairseq_cli.interactive | Total time: 1049.844 seconds; translation time: 1033.226\n",
      "2022-08-06 16:12:39 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 19, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-06 16:12:40 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-06 16:12:40 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-06 16:12:40 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto/checkpoint_best.pt\n",
      "2022-08-06 16:12:50 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-06 16:12:50 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-06 16:30:55 | INFO | fairseq_cli.interactive | Total time: 1095.675 seconds; translation time: 1078.818\n",
      "2022-08-06 16:31:00 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 20, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-06 16:31:00 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-06 16:31:00 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-06 16:31:00 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto/checkpoint_best.pt\n",
      "2022-08-06 16:31:10 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-06 16:31:10 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-06 16:50:17 | INFO | fairseq_cli.interactive | Total time: 1157.905 seconds; translation time: 1141.230\n"
     ]
    }
   ],
   "source": [
    "for N in range(1,21):\n",
    "    !fairseq-interactive kyoto/kyoto-bin \\\n",
    "        --path checkpoints/kyoto/checkpoint_best.pt \\\n",
    "        --input kyoto/kyoto-test.ja \\\n",
    "        --task translation \\\n",
    "        --beam {N} \\\n",
    "        | grep '^H' | cut -f3 > 94/beam{N}.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for N in range(1,21):\n",
    "    s = !fairseq-score --sys 94/beam{N}.out --ref kyoto/kyoto-test.en | grep '^B' | cut -c 9-12\n",
    "    scores.append(float(s[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = range(1, 21)\n",
    "plt.plot(x, scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 95. サブワード化\n",
    "トークンの単位を単語や形態素からサブワードに変更し，91-94の実験を再度実施せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=kftt-data-1.0/data/orig/kyoto-train.ja --model_prefix=kyoto_ja --vocab_size=16000 --character_coverage=1.0\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: kftt-data-1.0/data/orig/kyoto-train.ja\n",
      "  input_format: \n",
      "  model_prefix: kyoto_ja\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 16000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: kftt-data-1.0/data/orig/kyoto-train.ja\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 440288 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=17202261\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=6265\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 440288 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 1000000 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 440288\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 427567\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 427567 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=576376 obj=151.839 num_tokens=6375624 num_tokens/piece=11.0616\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=495331 obj=137.285 num_tokens=6425081 num_tokens/piece=12.9713\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=370678 obj=137.498 num_tokens=6564516 num_tokens/piece=17.7095\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=369061 obj=136.98 num_tokens=6592997 num_tokens/piece=17.8642\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=276675 obj=138.432 num_tokens=6767412 num_tokens/piece=24.4598\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=276538 obj=137.862 num_tokens=6771762 num_tokens/piece=24.4876\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=207398 obj=140.046 num_tokens=6991612 num_tokens/piece=33.7111\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=207382 obj=139.362 num_tokens=6996789 num_tokens/piece=33.7387\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=155535 obj=142.134 num_tokens=7239276 num_tokens/piece=46.5444\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=155535 obj=141.409 num_tokens=7241953 num_tokens/piece=46.5616\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=116651 obj=144.678 num_tokens=7505813 num_tokens/piece=64.3442\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=116651 obj=143.948 num_tokens=7508286 num_tokens/piece=64.3654\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=87488 obj=147.614 num_tokens=7790306 num_tokens/piece=89.0443\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=87488 obj=146.874 num_tokens=7791997 num_tokens/piece=89.0636\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=65616 obj=150.877 num_tokens=8092681 num_tokens/piece=123.334\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=65616 obj=150.118 num_tokens=8093329 num_tokens/piece=123.344\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=49212 obj=154.451 num_tokens=8423912 num_tokens/piece=171.176\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=49212 obj=153.621 num_tokens=8425514 num_tokens/piece=171.209\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=36909 obj=158.315 num_tokens=8782692 num_tokens/piece=237.955\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=36909 obj=157.405 num_tokens=8783568 num_tokens/piece=237.979\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=27681 obj=162.574 num_tokens=9191052 num_tokens/piece=332.035\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=27681 obj=161.538 num_tokens=9192813 num_tokens/piece=332.098\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=20760 obj=167.37 num_tokens=9660908 num_tokens/piece=465.362\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=20760 obj=166.178 num_tokens=9661610 num_tokens/piece=465.395\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=17600 obj=169.855 num_tokens=9958883 num_tokens/piece=565.846\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=17600 obj=169.124 num_tokens=9959901 num_tokens/piece=565.903\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: kyoto_ja.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: kyoto_ja.vocab\n"
     ]
    }
   ],
   "source": [
    "# 日本語のサブワード化\n",
    "import sentencepiece as spm\n",
    "import re\n",
    "\n",
    "spm.SentencePieceTrainer.Train('--input=kftt-data-1.0/data/orig/kyoto-train.ja --model_prefix=kyoto_ja --vocab_size=16000 --character_coverage=1.0')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('kyoto_ja.model')\n",
    "\n",
    "for src, dst in [\n",
    "    ('kftt-data-1.0/data/orig/kyoto-train.ja', 'kyoto/kyoto_sub-train.ja'),\n",
    "    ('kftt-data-1.0/data/orig/kyoto-dev.ja', 'kyoto/kyoto_sub-dev.ja'),\n",
    "    ('kftt-data-1.0/data/orig/kyoto-test.ja', 'kyoto/kyoto_sub-test.ja'),\n",
    "]:\n",
    "    with open(src) as f, open(dst, 'w') as g:\n",
    "        for x in f:\n",
    "            x = x.strip()\n",
    "            x = re.sub(r'\\s+', ' ', x)\n",
    "            x = sp.encode_as_pieces(x)\n",
    "            x = ' '.join(x)\n",
    "            print(x, file=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|####################################| 16000/16000 [01:06<00:00, 242.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# 英語のサブワード化\n",
    "!subword-nmt learn-bpe -s 16000 < kftt-data-1.0/data/orig/kyoto-train.en > kyoto_en.codes\n",
    "!subword-nmt apply-bpe -c kyoto_en.codes < kftt-data-1.0/data/orig/kyoto-train.en > kyoto/kyoto_sub-train.en\n",
    "!subword-nmt apply-bpe -c kyoto_en.codes < kftt-data-1.0/data/orig/kyoto-dev.en > kyoto/kyoto_sub-dev.en\n",
    "!subword-nmt apply-bpe -c kyoto_en.codes < kftt-data-1.0/data/orig/kyoto-test.en > kyoto/kyoto_sub-test.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--bpm sentencepieceをつけてもトークン数は変わらなかった\n",
    "```\n",
    "2022-08-06 22:08:17 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 146832 types\n",
    "2022-08-06 22:09:45 | INFO | fairseq_cli.preprocess | [ja] kyoto/kyoto-train.ja: 440288 sents, 12359715 tokens, 0.0% replaced (by <unk>)\n",
    "2022-08-06 22:09:45 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 146832 types\n",
    "2022-08-06 22:09:45 | INFO | fairseq_cli.preprocess | [ja] kyoto/kyoto-dev.ja: 1166 sents, 28010 tokens, 0.518% replaced (by <unk>)\n",
    "2022-08-06 22:09:45 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 146832 types\n",
    "2022-08-06 22:09:46 | INFO | fairseq_cli.preprocess | [ja] kyoto/kyoto-test.ja: 1160 sents, 29638 tokens, 0.55% replaced (by <unk>)\n",
    "2022-08-06 22:09:46 | INFO | fairseq_cli.preprocess | [en] Dictionary: 221864 types\n",
    "2022-08-06 22:10:48 | INFO | fairseq_cli.preprocess | [en] kyoto/kyoto-train.en: 440288 sents, 11981667 tokens, 0.0% replaced (by <unk>)\n",
    "2022-08-06 22:10:48 | INFO | fairseq_cli.preprocess | [en] Dictionary: 221864 types\n",
    "2022-08-06 22:10:48 | INFO | fairseq_cli.preprocess | [en] kyoto/kyoto-dev.en: 1166 sents, 25475 tokens, 1.79% replaced (by <unk>)\n",
    "2022-08-06 22:10:48 | INFO | fairseq_cli.preprocess | [en] Dictionary: 221864 types\n",
    "2022-08-06 22:10:48 | INFO | fairseq_cli.preprocess | [en] kyoto/kyoto-test.en: 1160 sents, 27894 tokens, 1.55% replaced (by <unk>)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-06 22:24:39 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='kyoto/kyoto_sub-bin', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, simul_type=None, source_lang='ja', srcdict=None, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref='kyoto/kyoto_sub-test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer='space', tpu=False, trainpref='kyoto/kyoto_sub-train', use_plasma_view=False, user_dir=None, validpref='kyoto/kyoto_sub-dev', wandb_project=None, workers=1)\n",
      "2022-08-06 22:26:03 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 16000 types\n",
      "2022-08-06 22:27:34 | INFO | fairseq_cli.preprocess | [ja] kyoto/kyoto_sub-train.ja: 440288 sents, 10735573 tokens, 0.0% replaced (by <unk>)\n",
      "2022-08-06 22:27:34 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 16000 types\n",
      "2022-08-06 22:27:34 | INFO | fairseq_cli.preprocess | [ja] kyoto/kyoto_sub-dev.ja: 1166 sents, 24825 tokens, 0.0121% replaced (by <unk>)\n",
      "2022-08-06 22:27:34 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 16000 types\n",
      "2022-08-06 22:27:34 | INFO | fairseq_cli.preprocess | [ja] kyoto/kyoto_sub-test.ja: 1160 sents, 26821 tokens, 0.0% replaced (by <unk>)\n",
      "2022-08-06 22:27:34 | INFO | fairseq_cli.preprocess | [en] Dictionary: 20872 types\n",
      "2022-08-06 22:28:41 | INFO | fairseq_cli.preprocess | [en] kyoto/kyoto_sub-train.en: 440288 sents, 13280091 tokens, 0.0% replaced (by <unk>)\n",
      "2022-08-06 22:28:41 | INFO | fairseq_cli.preprocess | [en] Dictionary: 20872 types\n",
      "2022-08-06 22:28:41 | INFO | fairseq_cli.preprocess | [en] kyoto/kyoto_sub-dev.en: 1166 sents, 29011 tokens, 0.00345% replaced (by <unk>)\n",
      "2022-08-06 22:28:41 | INFO | fairseq_cli.preprocess | [en] Dictionary: 20872 types\n",
      "2022-08-06 22:28:41 | INFO | fairseq_cli.preprocess | [en] kyoto/kyoto_sub-test.en: 1160 sents, 31468 tokens, 0.00318% replaced (by <unk>)\n",
      "2022-08-06 22:28:41 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to kyoto/kyoto_sub-bin\n"
     ]
    }
   ],
   "source": [
    "!fairseq-preprocess --source-lang ja --target-lang en \\\n",
    "   --trainpref kyoto/kyoto_sub-train \\\n",
    "   --validpref kyoto/kyoto_sub-dev \\\n",
    "   --testpref kyoto/kyoto_sub-test \\\n",
    "   --tokenizer space \\\n",
    "   --destdir kyoto/kyoto_sub-bin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-06 22:28:46 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard/kyoto_sub', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/kyoto_sub', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='kyoto/kyoto_sub-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=10, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_src_tgt_embed=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/kyoto_sub', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir='tensorboard/kyoto_sub', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'kyoto/kyoto_sub-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-06 22:28:46 | INFO | fairseq.tasks.translation | [ja] dictionary: 16000 types\n",
      "2022-08-06 22:28:46 | INFO | fairseq.tasks.translation | [en] dictionary: 20872 types\n",
      "2022-08-06 22:28:47 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(16000, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(20872, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=20872, bias=False)\n",
      "  )\n",
      ")\n",
      "2022-08-06 22:28:47 | INFO | fairseq_cli.train | task: TranslationTask\n",
      "2022-08-06 22:28:47 | INFO | fairseq_cli.train | model: TransformerModel\n",
      "2022-08-06 22:28:47 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2022-08-06 22:28:47 | INFO | fairseq_cli.train | num. shared model params: 63,016,960 (num. trained: 63,016,960)\n",
      "2022-08-06 22:28:47 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2022-08-06 22:28:47 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: kyoto/kyoto_sub-bin/valid.ja-en.ja\n",
      "2022-08-06 22:28:47 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: kyoto/kyoto_sub-bin/valid.ja-en.en\n",
      "2022-08-06 22:28:47 | INFO | fairseq.tasks.translation | kyoto/kyoto_sub-bin valid ja-en 1166 examples\n",
      "2022-08-06 22:28:51 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2022-08-06 22:28:51 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-08-06 22:28:51 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 47.544 GB ; name = NVIDIA RTX A6000                        \n",
      "2022-08-06 22:28:51 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-08-06 22:28:51 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2022-08-06 22:28:51 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
      "2022-08-06 22:28:51 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/kyoto_sub/checkpoint_last.pt\n",
      "2022-08-06 22:28:51 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/kyoto_sub/checkpoint_last.pt\n",
      "2022-08-06 22:28:51 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2022-08-06 22:28:51 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: kyoto/kyoto_sub-bin/train.ja-en.ja\n",
      "2022-08-06 22:28:51 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: kyoto/kyoto_sub-bin/train.ja-en.en\n",
      "2022-08-06 22:28:51 | INFO | fairseq.tasks.translation | kyoto/kyoto_sub-bin train ja-en 440288 examples\n",
      "2022-08-06 22:28:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3644\n",
      "epoch 001:   0%|                                       | 0/3644 [00:00<?, ?it/s]2022-08-06 22:28:51 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2022-08-06 22:28:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 001:   0%|                               | 3/3644 [00:00<14:48,  4.10it/s]2022-08-06 22:28:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 001:   0%|                               | 9/3644 [00:01<05:48, 10.44it/s]2022-08-06 22:28:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001:   5%| | 189/3644 [00:13<03:46, 15.25it/s, loss=13.935, nll_loss=13.802022-08-06 22:29:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 001: 100%|▉| 3643/3644 [04:03<00:00, 15.28it/s, loss=8.667, nll_loss=7.7272022-08-06 22:32:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   7%|▌      | 1/14 [00:00<00:07,  1.84it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  14%|█      | 2/14 [00:00<00:05,  2.20it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  21%|█▌     | 3/14 [00:01<00:04,  2.30it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  29%|██     | 4/14 [00:01<00:04,  2.28it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  36%|██▌    | 5/14 [00:02<00:04,  2.19it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  43%|███    | 6/14 [00:02<00:03,  2.08it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  50%|███▌   | 7/14 [00:03<00:03,  1.97it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  57%|████   | 8/14 [00:03<00:03,  1.85it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  64%|████▌  | 9/14 [00:04<00:02,  1.76it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  71%|████▎ | 10/14 [00:05<00:02,  1.60it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  79%|████▋ | 11/14 [00:06<00:02,  1.48it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  86%|█████▏| 12/14 [00:07<00:01,  1.37it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  93%|█████▌| 13/14 [00:08<00:00,  1.20it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|██████| 14/14 [00:09<00:00,  1.01it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-06 22:33:04 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.643 | nll_loss 7.646 | ppl 200.36 | bleu 1.12 | wps 3123.5 | wpb 2072.2 | bsz 83.3 | num_updates 3641\n",
      "2022-08-06 22:33:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 3641 updates\n",
      "2022-08-06 22:33:04 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_sub/checkpoint_best.pt\n",
      "2022-08-06 22:33:05 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_sub/checkpoint_best.pt\n",
      "2022-08-06 22:33:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_sub/checkpoint_best.pt (epoch 1 @ 3641 updates, score 1.12) (writing took 1.4879708839580417 seconds)\n",
      "2022-08-06 22:33:05 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2022-08-06 22:33:05 | INFO | train | epoch 001 | loss 10.079 | nll_loss 9.37 | ppl 661.56 | wps 52324.5 | ups 14.36 | wpb 3644 | bsz 120.3 | num_updates 3641 | lr 0.000455125 | gnorm 1.247 | loss_scale 16 | train_wall 235 | gb_free 44.4 | wall 254\n",
      "2022-08-06 22:33:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3644\n",
      "epoch 002:   0%|                                       | 0/3644 [00:00<?, ?it/s]2022-08-06 22:33:06 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2022-08-06 22:33:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002: 100%|▉| 3643/3644 [04:03<00:00, 15.43it/s, loss=7.542, nll_loss=6.4392022-08-06 22:37:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|               | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:   7%|▌      | 1/14 [00:00<00:06,  2.04it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  14%|█      | 2/14 [00:00<00:05,  2.38it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  21%|█▌     | 3/14 [00:01<00:04,  2.44it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  29%|██     | 4/14 [00:01<00:04,  2.39it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  36%|██▌    | 5/14 [00:02<00:03,  2.27it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  43%|███    | 6/14 [00:02<00:03,  2.15it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  50%|███▌   | 7/14 [00:03<00:03,  2.01it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  57%|████   | 8/14 [00:03<00:03,  1.87it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  64%|████▌  | 9/14 [00:04<00:02,  1.77it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  71%|████▎ | 10/14 [00:05<00:02,  1.66it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  79%|████▋ | 11/14 [00:05<00:01,  1.51it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  86%|█████▏| 12/14 [00:06<00:01,  1.39it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  93%|█████▌| 13/14 [00:07<00:00,  1.21it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|██████| 14/14 [00:09<00:00,  1.01it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-06 22:37:19 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.667 | nll_loss 6.503 | ppl 90.7 | bleu 1.76 | wps 3164.7 | wpb 2072.2 | bsz 83.3 | num_updates 7285 | best_bleu 1.76\n",
      "2022-08-06 22:37:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 7285 updates\n",
      "2022-08-06 22:37:19 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_sub/checkpoint_best.pt\n",
      "2022-08-06 22:37:20 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_sub/checkpoint_best.pt\n",
      "2022-08-06 22:37:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_sub/checkpoint_best.pt (epoch 2 @ 7285 updates, score 1.76) (writing took 1.8639341189991683 seconds)\n",
      "2022-08-06 22:37:20 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2022-08-06 22:37:20 | INFO | train | epoch 002 | loss 7.931 | nll_loss 6.884 | ppl 118.11 | wps 52091.6 | ups 14.29 | wpb 3644.4 | bsz 120.8 | num_updates 7285 | lr 0.000370497 | gnorm 0.918 | loss_scale 16 | train_wall 235 | gb_free 44.7 | wall 509\n",
      "2022-08-06 22:37:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3644\n",
      "epoch 003:   0%|                                       | 0/3644 [00:00<?, ?it/s]2022-08-06 22:37:21 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2022-08-06 22:37:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003:  72%|▋| 2609/3644 [02:55<01:10, 14.66it/s, loss=7.228, nll_loss=6.0792022-08-06 22:40:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 003: 100%|▉| 3643/3644 [04:05<00:00, 15.58it/s, loss=7.14, nll_loss=5.98, 2022-08-06 22:41:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|               | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:   7%|▌      | 1/14 [00:00<00:06,  2.00it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  14%|█      | 2/14 [00:00<00:05,  2.35it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  21%|█▌     | 3/14 [00:01<00:04,  2.43it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  29%|██     | 4/14 [00:01<00:04,  2.40it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  36%|██▌    | 5/14 [00:02<00:03,  2.29it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  43%|███    | 6/14 [00:02<00:03,  2.17it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  50%|███▌   | 7/14 [00:03<00:03,  2.02it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  57%|████   | 8/14 [00:03<00:03,  1.87it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  64%|████▌  | 9/14 [00:04<00:02,  1.78it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  71%|████▎ | 10/14 [00:05<00:02,  1.67it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  79%|████▋ | 11/14 [00:05<00:01,  1.52it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  86%|█████▏| 12/14 [00:06<00:01,  1.40it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  93%|█████▌| 13/14 [00:07<00:00,  1.22it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|██████| 14/14 [00:09<00:00,  1.01it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-06 22:41:35 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.344 | nll_loss 6.135 | ppl 70.27 | bleu 2.43 | wps 3181.4 | wpb 2072.2 | bsz 83.3 | num_updates 10928 | best_bleu 2.43\n",
      "2022-08-06 22:41:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 10928 updates\n",
      "2022-08-06 22:41:35 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_sub/checkpoint_best.pt\n",
      "2022-08-06 22:41:36 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_sub/checkpoint_best.pt\n",
      "2022-08-06 22:41:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_sub/checkpoint_best.pt (epoch 3 @ 10928 updates, score 2.43) (writing took 1.8518112138845026 seconds)\n",
      "2022-08-06 22:41:37 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2022-08-06 22:41:37 | INFO | train | epoch 003 | loss 7.283 | nll_loss 6.143 | ppl 70.66 | wps 51747.1 | ups 14.2 | wpb 3644.3 | bsz 120.5 | num_updates 10928 | lr 0.000302503 | gnorm 0.885 | loss_scale 8 | train_wall 237 | gb_free 44.7 | wall 766\n",
      "2022-08-06 22:41:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3644\n",
      "epoch 004:   0%|                                       | 0/3644 [00:00<?, ?it/s]2022-08-06 22:41:37 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2022-08-06 22:41:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004: 100%|▉| 3643/3644 [04:04<00:00, 15.64it/s, loss=6.913, nll_loss=5.7212022-08-06 22:45:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|               | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:   7%|▌      | 1/14 [00:00<00:06,  2.02it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  14%|█      | 2/14 [00:00<00:05,  2.13it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  21%|█▌     | 3/14 [00:01<00:04,  2.27it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  29%|██     | 4/14 [00:01<00:04,  2.26it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  36%|██▌    | 5/14 [00:02<00:04,  2.18it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  43%|███    | 6/14 [00:02<00:03,  2.06it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  50%|███▌   | 7/14 [00:03<00:03,  1.94it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  57%|████   | 8/14 [00:04<00:03,  1.81it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  64%|████▌  | 9/14 [00:04<00:02,  1.72it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  71%|████▎ | 10/14 [00:05<00:02,  1.61it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  79%|████▋ | 11/14 [00:06<00:02,  1.47it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  86%|█████▏| 12/14 [00:07<00:01,  1.35it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  93%|█████▌| 13/14 [00:08<00:00,  1.18it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|██████| 14/14 [00:09<00:00,  1.01s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-06 22:45:51 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.156 | nll_loss 5.914 | ppl 60.28 | bleu 2.59 | wps 3066.7 | wpb 2072.2 | bsz 83.3 | num_updates 14572 | best_bleu 2.59\n",
      "2022-08-06 22:45:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 14572 updates\n",
      "2022-08-06 22:45:51 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_sub/checkpoint_best.pt\n",
      "epoch 004: 100%|▉| 3643/3644 [04:14<00:00, 15.64it/s, loss=6.913, nll_loss=5.7212022-08-06 22:45:52 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_sub/checkpoint_best.pt\n",
      "2022-08-06 22:45:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_sub/checkpoint_best.pt (epoch 4 @ 14572 updates, score 2.59) (writing took 1.8675247391220182 seconds)\n",
      "2022-08-06 22:45:53 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2022-08-06 22:45:53 | INFO | train | epoch 004 | loss 6.997 | nll_loss 5.816 | ppl 56.32 | wps 51919.4 | ups 14.25 | wpb 3644.4 | bsz 120.8 | num_updates 14572 | lr 0.000261963 | gnorm 0.92 | loss_scale 8 | train_wall 236 | gb_free 44.4 | wall 1022\n",
      "2022-08-06 22:45:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3644\n",
      "epoch 005:   0%|                                       | 0/3644 [00:00<?, ?it/s]2022-08-06 22:45:53 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2022-08-06 22:45:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005: 100%|▉| 3643/3644 [04:02<00:00, 16.03it/s, loss=6.752, nll_loss=5.5352022-08-06 22:49:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|               | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:   7%|▌      | 1/14 [00:00<00:06,  2.07it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  14%|█      | 2/14 [00:00<00:04,  2.41it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  21%|█▌     | 3/14 [00:01<00:04,  2.50it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  29%|██     | 4/14 [00:01<00:04,  2.44it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  36%|██▌    | 5/14 [00:02<00:03,  2.32it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  43%|███    | 6/14 [00:02<00:03,  2.20it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  50%|███▌   | 7/14 [00:03<00:03,  2.06it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  57%|████   | 8/14 [00:03<00:03,  1.90it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  64%|████▌  | 9/14 [00:04<00:02,  1.80it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  71%|████▎ | 10/14 [00:05<00:02,  1.68it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  79%|████▋ | 11/14 [00:05<00:01,  1.53it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  86%|█████▏| 12/14 [00:06<00:01,  1.40it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  93%|█████▌| 13/14 [00:07<00:00,  1.22it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|██████| 14/14 [00:09<00:00,  1.02it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-06 22:50:05 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.07 | nll_loss 5.819 | ppl 56.47 | bleu 2.48 | wps 3204.2 | wpb 2072.2 | bsz 83.3 | num_updates 18216 | best_bleu 2.59\n",
      "2022-08-06 22:50:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 18216 updates\n",
      "2022-08-06 22:50:05 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_sub/checkpoint_last.pt\n",
      "2022-08-06 22:50:06 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_sub/checkpoint_last.pt\n",
      "2022-08-06 22:50:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_sub/checkpoint_last.pt (epoch 5 @ 18216 updates, score 2.48) (writing took 1.0335969019215554 seconds)\n",
      "2022-08-06 22:50:06 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2022-08-06 22:50:06 | INFO | train | epoch 005 | loss 6.822 | nll_loss 5.615 | ppl 48.99 | wps 52419.3 | ups 14.38 | wpb 3644.4 | bsz 120.8 | num_updates 18216 | lr 0.000234301 | gnorm 0.963 | loss_scale 8 | train_wall 235 | gb_free 44.6 | wall 1275\n",
      "2022-08-06 22:50:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3644\n",
      "epoch 006:   0%|                                       | 0/3644 [00:00<?, ?it/s]2022-08-06 22:50:06 | INFO | fairseq.trainer | begin training epoch 6\n",
      "2022-08-06 22:50:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 006: 100%|▉| 3643/3644 [04:02<00:00, 15.74it/s, loss=6.617, nll_loss=5.38,2022-08-06 22:54:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|               | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:   7%|▌      | 1/14 [00:00<00:06,  2.07it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  14%|█      | 2/14 [00:00<00:04,  2.41it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  21%|█▌     | 3/14 [00:01<00:04,  2.46it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  29%|██     | 4/14 [00:01<00:04,  2.41it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  36%|██▌    | 5/14 [00:02<00:03,  2.30it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  43%|███    | 6/14 [00:02<00:03,  2.17it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  50%|███▌   | 7/14 [00:03<00:03,  2.03it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  57%|████   | 8/14 [00:03<00:03,  1.89it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  64%|████▌  | 9/14 [00:04<00:02,  1.79it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  71%|████▎ | 10/14 [00:05<00:02,  1.67it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  79%|████▋ | 11/14 [00:05<00:01,  1.53it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  86%|█████▏| 12/14 [00:06<00:01,  1.41it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  93%|█████▌| 13/14 [00:07<00:00,  1.26it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|██████| 14/14 [00:08<00:00,  1.07it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-06 22:54:18 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.021 | nll_loss 5.75 | ppl 53.81 | bleu 2.42 | wps 3262.4 | wpb 2072.2 | bsz 83.3 | num_updates 21860 | best_bleu 2.59\n",
      "2022-08-06 22:54:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 21860 updates\n",
      "2022-08-06 22:54:18 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_sub/checkpoint_last.pt\n",
      "2022-08-06 22:54:19 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_sub/checkpoint_last.pt\n",
      "2022-08-06 22:54:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_sub/checkpoint_last.pt (epoch 6 @ 21860 updates, score 2.42) (writing took 1.0234364850912243 seconds)\n",
      "2022-08-06 22:54:19 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2022-08-06 22:54:19 | INFO | train | epoch 006 | loss 6.697 | nll_loss 5.47 | ppl 44.33 | wps 52517.4 | ups 14.41 | wpb 3644.4 | bsz 120.8 | num_updates 21860 | lr 0.000213882 | gnorm 0.995 | loss_scale 8 | train_wall 234 | gb_free 44.6 | wall 1528\n",
      "2022-08-06 22:54:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3644\n",
      "epoch 007:   0%|                                       | 0/3644 [00:00<?, ?it/s]2022-08-06 22:54:19 | INFO | fairseq.trainer | begin training epoch 7\n",
      "2022-08-06 22:54:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 007: 100%|▉| 3643/3644 [04:03<00:00, 15.83it/s, loss=6.586, nll_loss=5.3432022-08-06 22:58:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|               | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:   7%|▌      | 1/14 [00:00<00:06,  2.14it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  14%|█      | 2/14 [00:00<00:04,  2.47it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  21%|█▌     | 3/14 [00:01<00:04,  2.52it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  29%|██     | 4/14 [00:01<00:04,  2.45it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  36%|██▌    | 5/14 [00:02<00:03,  2.33it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  43%|███    | 6/14 [00:02<00:03,  2.06it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  50%|███▌   | 7/14 [00:03<00:03,  1.96it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  57%|████   | 8/14 [00:03<00:03,  1.84it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  64%|████▌  | 9/14 [00:04<00:02,  1.75it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  71%|████▎ | 10/14 [00:05<00:02,  1.64it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  79%|████▋ | 11/14 [00:06<00:02,  1.50it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  86%|█████▏| 12/14 [00:06<00:01,  1.38it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  93%|█████▌| 13/14 [00:07<00:00,  1.21it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|██████| 14/14 [00:09<00:00,  1.04it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-06 22:58:32 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.988 | nll_loss 5.711 | ppl 52.4 | bleu 2.5 | wps 3181.9 | wpb 2072.2 | bsz 83.3 | num_updates 25504 | best_bleu 2.59\n",
      "2022-08-06 22:58:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 25504 updates\n",
      "2022-08-06 22:58:32 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_sub/checkpoint_last.pt\n",
      "2022-08-06 22:58:33 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_sub/checkpoint_last.pt\n",
      "2022-08-06 22:58:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_sub/checkpoint_last.pt (epoch 7 @ 25504 updates, score 2.5) (writing took 1.0339215060230345 seconds)\n",
      "2022-08-06 22:58:33 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
      "2022-08-06 22:58:33 | INFO | train | epoch 007 | loss 6.601 | nll_loss 5.36 | ppl 41.07 | wps 52206.3 | ups 14.33 | wpb 3644.4 | bsz 120.8 | num_updates 25504 | lr 0.000198014 | gnorm 1.027 | loss_scale 8 | train_wall 235 | gb_free 44.7 | wall 1782\n",
      "2022-08-06 22:58:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3644\n",
      "epoch 008:   0%|                                       | 0/3644 [00:00<?, ?it/s]2022-08-06 22:58:33 | INFO | fairseq.trainer | begin training epoch 8\n",
      "2022-08-06 22:58:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 008:  77%|▊| 2799/3644 [03:06<00:56, 15.00it/s, loss=6.566, nll_loss=5.3192022-08-06 23:01:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 008: 100%|▉| 3643/3644 [04:02<00:00, 15.89it/s, loss=6.639, nll_loss=5.4022022-08-06 23:02:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|               | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:   7%|▌      | 1/14 [00:00<00:06,  2.07it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  14%|█      | 2/14 [00:00<00:04,  2.42it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  21%|█▌     | 3/14 [00:01<00:04,  2.52it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  29%|██     | 4/14 [00:01<00:04,  2.46it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  36%|██▌    | 5/14 [00:02<00:03,  2.34it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  43%|███    | 6/14 [00:02<00:03,  2.20it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  50%|███▌   | 7/14 [00:03<00:03,  2.06it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  57%|████   | 8/14 [00:03<00:03,  1.90it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  64%|████▌  | 9/14 [00:04<00:02,  1.80it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  71%|████▎ | 10/14 [00:05<00:02,  1.68it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  79%|████▋ | 11/14 [00:05<00:01,  1.54it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  86%|█████▏| 12/14 [00:06<00:01,  1.41it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  93%|█████▌| 13/14 [00:07<00:00,  1.23it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|██████| 14/14 [00:09<00:00,  1.02it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-06 23:02:45 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.959 | nll_loss 5.679 | ppl 51.25 | bleu 2.28 | wps 3214.9 | wpb 2072.2 | bsz 83.3 | num_updates 29147 | best_bleu 2.59\n",
      "2022-08-06 23:02:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 29147 updates\n",
      "2022-08-06 23:02:45 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_sub/checkpoint_last.pt\n",
      "2022-08-06 23:02:46 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_sub/checkpoint_last.pt\n",
      "2022-08-06 23:02:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_sub/checkpoint_last.pt (epoch 8 @ 29147 updates, score 2.28) (writing took 1.03238989203237 seconds)\n",
      "2022-08-06 23:02:46 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
      "2022-08-06 23:02:46 | INFO | train | epoch 008 | loss 6.528 | nll_loss 5.276 | ppl 38.74 | wps 52501.7 | ups 14.41 | wpb 3644.3 | bsz 120.5 | num_updates 29147 | lr 0.000185226 | gnorm 1.061 | loss_scale 8 | train_wall 234 | gb_free 44.7 | wall 2035\n",
      "2022-08-06 23:02:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3644\n",
      "epoch 009:   0%|                                       | 0/3644 [00:00<?, ?it/s]2022-08-06 23:02:46 | INFO | fairseq.trainer | begin training epoch 9\n",
      "2022-08-06 23:02:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 009: 100%|▉| 3643/3644 [04:02<00:00, 15.49it/s, loss=6.495, nll_loss=5.2382022-08-06 23:06:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|               | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:   7%|▌      | 1/14 [00:00<00:06,  2.06it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  14%|█      | 2/14 [00:00<00:04,  2.42it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  21%|█▌     | 3/14 [00:01<00:04,  2.27it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  29%|██     | 4/14 [00:01<00:04,  2.30it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  36%|██▌    | 5/14 [00:02<00:04,  2.24it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  43%|███    | 6/14 [00:02<00:03,  2.14it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  50%|███▌   | 7/14 [00:03<00:03,  2.01it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  57%|████   | 8/14 [00:03<00:03,  1.87it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  64%|████▌  | 9/14 [00:04<00:02,  1.78it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  71%|████▎ | 10/14 [00:05<00:02,  1.66it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  79%|████▋ | 11/14 [00:05<00:01,  1.52it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  86%|█████▏| 12/14 [00:06<00:01,  1.39it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  93%|█████▌| 13/14 [00:07<00:00,  1.21it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|██████| 14/14 [00:08<00:00,  1.18it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-06 23:06:57 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.894 | nll_loss 5.616 | ppl 49.06 | bleu 3.02 | wps 3325.5 | wpb 2072.2 | bsz 83.3 | num_updates 32791 | best_bleu 3.02\n",
      "2022-08-06 23:06:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 32791 updates\n",
      "2022-08-06 23:06:57 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_sub/checkpoint_best.pt\n",
      "2022-08-06 23:06:58 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_sub/checkpoint_best.pt\n",
      "2022-08-06 23:06:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_sub/checkpoint_best.pt (epoch 9 @ 32791 updates, score 3.02) (writing took 1.828881154069677 seconds)\n",
      "2022-08-06 23:06:59 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
      "2022-08-06 23:06:59 | INFO | train | epoch 009 | loss 6.466 | nll_loss 5.204 | ppl 36.85 | wps 52504.6 | ups 14.41 | wpb 3644.4 | bsz 120.8 | num_updates 32791 | lr 0.000174632 | gnorm 1.087 | loss_scale 8 | train_wall 234 | gb_free 44.5 | wall 2288\n",
      "2022-08-06 23:06:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3644\n",
      "epoch 010:   0%|                                       | 0/3644 [00:00<?, ?it/s]2022-08-06 23:06:59 | INFO | fairseq.trainer | begin training epoch 10\n",
      "2022-08-06 23:06:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 010: 100%|▉| 3643/3644 [04:02<00:00, 15.24it/s, loss=6.355, nll_loss=5.0772022-08-06 23:11:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|               | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:   7%|▌      | 1/14 [00:00<00:06,  2.07it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  14%|█      | 2/14 [00:00<00:04,  2.43it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  21%|█▌     | 3/14 [00:01<00:04,  2.51it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  29%|██     | 4/14 [00:01<00:04,  2.44it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  36%|██▌    | 5/14 [00:02<00:03,  2.32it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  43%|███    | 6/14 [00:02<00:03,  2.19it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  50%|███▌   | 7/14 [00:03<00:03,  2.04it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  57%|████   | 8/14 [00:03<00:03,  1.89it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  64%|████▌  | 9/14 [00:04<00:02,  1.79it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  71%|████▎ | 10/14 [00:05<00:02,  1.67it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  79%|████▋ | 11/14 [00:05<00:01,  1.52it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  86%|█████▏| 12/14 [00:06<00:01,  1.39it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  93%|█████▌| 13/14 [00:07<00:00,  1.21it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|██████| 14/14 [00:09<00:00,  1.01it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-08-06 23:11:11 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.886 | nll_loss 5.602 | ppl 48.58 | bleu 3.01 | wps 3184.2 | wpb 2072.2 | bsz 83.3 | num_updates 36435 | best_bleu 3.02\n",
      "2022-08-06 23:11:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 36435 updates\n",
      "2022-08-06 23:11:11 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_sub/checkpoint_last.pt\n",
      "2022-08-06 23:11:12 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_sub/checkpoint_last.pt\n",
      "2022-08-06 23:11:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_sub/checkpoint_last.pt (epoch 10 @ 36435 updates, score 3.01) (writing took 1.0273022612091154 seconds)\n",
      "2022-08-06 23:11:12 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
      "2022-08-06 23:11:12 | INFO | train | epoch 010 | loss 6.415 | nll_loss 5.145 | ppl 35.38 | wps 52491.3 | ups 14.4 | wpb 3644.4 | bsz 120.8 | num_updates 36435 | lr 0.000165669 | gnorm 1.117 | loss_scale 8 | train_wall 234 | gb_free 44.6 | wall 2541\n",
      "2022-08-06 23:11:12 | INFO | fairseq_cli.train | done training in 2541.0 seconds\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 fairseq-train kyoto/kyoto_sub-bin \\\n",
    "    --save-dir checkpoints/kyoto_sub \\\n",
    "    --tensorboard-logdir tensorboard/kyoto_sub \\\n",
    "    --arch transformer --task translation \\\n",
    "\t--share-decoder-input-output-embed \\\n",
    " \t--optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "\t--lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "\t--criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "\t--dropout 0.3 --weight-decay 0.0001 \\\n",
    "\t--max-tokens 4096 \\\n",
    "    --eval-bleu \\\n",
    "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
    "    --eval-bleu-detok space \\\n",
    "    --eval-bleu-remove-bpe \\\n",
    "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
    "    --max-epoch 10 \\\n",
    "\t--fp16 \\\n",
    "    --no-epoch-checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-06 23:11:17 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto_sub/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto_sub-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto_sub-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-06 23:11:17 | INFO | fairseq.tasks.translation | [ja] dictionary: 16000 types\n",
      "2022-08-06 23:11:17 | INFO | fairseq.tasks.translation | [en] dictionary: 20872 types\n",
      "2022-08-06 23:11:17 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto_sub/checkpoint_best.pt\n",
      "2022-08-06 23:11:22 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-06 23:11:22 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-06 23:15:26 | INFO | fairseq_cli.interactive | Total time: 248.462 seconds; translation time: 237.997\n"
     ]
    }
   ],
   "source": [
    "!fairseq-interactive kyoto/kyoto_sub-bin \\\n",
    "    --path checkpoints/kyoto_sub/checkpoint_best.pt  \\\n",
    "    --input kyoto/kyoto_sub-test.ja \\\n",
    "    --task translation \\\n",
    "    | grep '^H' | cut -f3 > 95.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat 95.out | sed -r 's/(@@ )|(@@ ?$)//g' > 95-1.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(ignore_case=False, order=4, ref='kyoto/kyoto-test.en', sacrebleu=False, sentence_bleu=False, sys='95-1.out')\n",
      "BLEU4 = 1.76, 18.1/3.5/0.8/0.3 (BP=0.877, ratio=0.884, syslen=23640, reflen=26734)\n"
     ]
    }
   ],
   "source": [
    "!fairseq-score --sys 95-1.out --ref kyoto/kyoto-test.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 96. 学習過程の可視化\n",
    "Tensorboardなどのツールを用い，ニューラル機械翻訳モデルが学習されていく過程を可視化せよ．可視化する項目としては，学習データにおける損失関数の値とBLEUスコア，開発データにおける損失関数の値とBLEUスコアなどを採用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./tensorboard/kyoto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![loss](./96result/96_loss.png)\n",
    "\n",
    "![loss](./96result/96_blue.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 97. ハイパー・パラメータの調整\n",
    "ニューラルネットワークのモデルや，そのハイパーパラメータを変更しつつ，開発データにおけるBLEUスコアが最大となるモデルとハイパーパラメータを求めよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-07 14:09:09 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard/kyoto_97//lr1e-05_drop0.2', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/kyoto_97/lr1e-05_drop0.2', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='kyoto/kyoto-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.2, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[1e-05], lr_scheduler='inverse_sqrt', max_epoch=10, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_src_tgt_embed=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/kyoto_97/lr1e-05_drop0.2', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir='tensorboard/kyoto_97//lr1e-05_drop0.2', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-07 14:09:09 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-07 14:09:09 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-07 14:09:14 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(146832, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(221864, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=221864, bias=False)\n",
      "  )\n",
      ")\n",
      "2022-08-07 14:09:14 | INFO | fairseq_cli.train | task: TranslationTask\n",
      "2022-08-07 14:09:14 | INFO | fairseq_cli.train | model: TransformerModel\n",
      "2022-08-07 14:09:14 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2022-08-07 14:09:14 | INFO | fairseq_cli.train | num. shared model params: 232,910,848 (num. trained: 232,910,848)\n",
      "2022-08-07 14:09:14 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2022-08-07 14:09:14 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: kyoto/kyoto-bin/valid.ja-en.ja\n",
      "2022-08-07 14:09:14 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: kyoto/kyoto-bin/valid.ja-en.en\n",
      "2022-08-07 14:09:14 | INFO | fairseq.tasks.translation | kyoto/kyoto-bin valid ja-en 1166 examples\n",
      "2022-08-07 14:09:19 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2022-08-07 14:09:19 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-08-07 14:09:19 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 47.544 GB ; name = NVIDIA RTX A6000                        \n",
      "2022-08-07 14:09:19 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-08-07 14:09:19 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2022-08-07 14:09:19 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
      "2022-08-07 14:09:19 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/kyoto_97/lr1e-05_drop0.2/checkpoint_last.pt\n",
      "2022-08-07 14:09:23 | INFO | fairseq.trainer | Loaded checkpoint checkpoints/kyoto_97/lr1e-05_drop0.2/checkpoint_last.pt (epoch 10 @ 32387 updates)\n",
      "2022-08-07 14:09:23 | INFO | fairseq.trainer | loading train data for epoch 10\n",
      "2022-08-07 14:09:23 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: kyoto/kyoto-bin/train.ja-en.ja\n",
      "2022-08-07 14:09:23 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: kyoto/kyoto-bin/train.ja-en.en\n",
      "2022-08-07 14:09:23 | INFO | fairseq.tasks.translation | kyoto/kyoto-bin train ja-en 440288 examples\n",
      "2022-08-07 14:09:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 010:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 14:09:23 | INFO | fairseq.trainer | begin training epoch 10\n",
      "2022-08-07 14:09:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 010: 100%|▉| 3598/3599 [09:11<00:00,  7.13it/s, loss=8.596, nll_loss=7.2562022-08-07 14:18:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:11,  1.05it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:09,  1.16it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.12it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:08,  1.07it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:08,  1.02s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:06<00:07,  1.13s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:07,  1.21s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:09<00:06,  1.33s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.41s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:12<00:04,  1.56s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  85%|█████ | 11/13 [00:14<00:03,  1.75s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:17<00:02,  2.05s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|██████| 13/13 [00:20<00:00,  2.43s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 14:18:56 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.791 | nll_loss 7.454 | ppl 175.28 | bleu 2.92 | wps 1223.5 | wpb 1959.6 | bsz 89.7 | num_updates 35986 | best_bleu 3.03\n",
      "2022-08-07 14:18:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 35986 updates\n",
      "2022-08-07 14:18:56 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr1e-05_drop0.2/checkpoint_last.pt\n",
      "2022-08-07 14:19:01 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr1e-05_drop0.2/checkpoint_last.pt\n",
      "2022-08-07 14:19:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr1e-05_drop0.2/checkpoint_last.pt (epoch 10 @ 35986 updates, score 2.92) (writing took 4.949625026900321 seconds)\n",
      "2022-08-07 14:19:01 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
      "2022-08-07 14:19:01 | INFO | train | epoch 010 | loss 8.615 | nll_loss 7.276 | ppl 155.02 | wps 20763.4 | ups 6.24 | wpb 3329.2 | bsz 122.3 | num_updates 35986 | lr 3.33398e-06 | gnorm 1.916 | loss_scale 16 | train_wall 543 | gb_free 33.5 | wall 582\n",
      "2022-08-07 14:19:01 | INFO | fairseq_cli.train | done training in 577.9 seconds\n",
      "2022-08-07 14:19:06 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard/kyoto_97//lr1e-05_drop0.3', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/kyoto_97/lr1e-05_drop0.3', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='kyoto/kyoto-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[1e-05], lr_scheduler='inverse_sqrt', max_epoch=10, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_src_tgt_embed=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/kyoto_97/lr1e-05_drop0.3', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir='tensorboard/kyoto_97//lr1e-05_drop0.3', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-07 14:19:06 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-07 14:19:06 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-07 14:19:11 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(146832, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(221864, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=221864, bias=False)\n",
      "  )\n",
      ")\n",
      "2022-08-07 14:19:11 | INFO | fairseq_cli.train | task: TranslationTask\n",
      "2022-08-07 14:19:11 | INFO | fairseq_cli.train | model: TransformerModel\n",
      "2022-08-07 14:19:11 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2022-08-07 14:19:11 | INFO | fairseq_cli.train | num. shared model params: 232,910,848 (num. trained: 232,910,848)\n",
      "2022-08-07 14:19:11 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2022-08-07 14:19:11 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: kyoto/kyoto-bin/valid.ja-en.ja\n",
      "2022-08-07 14:19:11 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: kyoto/kyoto-bin/valid.ja-en.en\n",
      "2022-08-07 14:19:11 | INFO | fairseq.tasks.translation | kyoto/kyoto-bin valid ja-en 1166 examples\n",
      "2022-08-07 14:19:15 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2022-08-07 14:19:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-08-07 14:19:15 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 47.544 GB ; name = NVIDIA RTX A6000                        \n",
      "2022-08-07 14:19:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-08-07 14:19:15 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2022-08-07 14:19:15 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
      "2022-08-07 14:19:15 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_last.pt\n",
      "2022-08-07 14:19:15 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_last.pt\n",
      "2022-08-07 14:19:15 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2022-08-07 14:19:15 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: kyoto/kyoto-bin/train.ja-en.ja\n",
      "2022-08-07 14:19:15 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: kyoto/kyoto-bin/train.ja-en.en\n",
      "2022-08-07 14:19:15 | INFO | fairseq.tasks.translation | kyoto/kyoto-bin train ja-en 440288 examples\n",
      "2022-08-07 14:19:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 001:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 14:19:16 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2022-08-07 14:19:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 001:   1%|▏                             | 21/3599 [00:04<09:06,  6.55it/s]2022-08-07 14:19:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 001:   3%| | 119/3599 [00:19<09:22,  6.19it/s, loss=18.42, nll_loss=18.4142022-08-07 14:19:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001:  10%| | 359/3599 [00:55<08:09,  6.62it/s, loss=16.896, nll_loss=16.722022-08-07 14:20:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 001: 100%|▉| 3598/3599 [09:14<00:00,  6.66it/s, loss=10.733, nll_loss=9.642022-08-07 14:28:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:10,  1.18it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:08,  1.25it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.14it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:08,  1.04it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:08,  1.08s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:06<00:08,  1.16s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:07,  1.24s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:09<00:06,  1.38s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:11<00:05,  1.47s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:12<00:04,  1.62s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  85%|█████ | 11/13 [00:15<00:03,  1.83s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:18<00:02,  2.12s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|██████| 13/13 [00:21<00:00,  2.49s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 14:28:51 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.799 | nll_loss 9.682 | ppl 821.52 | bleu 0.12 | wps 1183.1 | wpb 1959.6 | bsz 89.7 | num_updates 3596\n",
      "2022-08-07 14:28:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 3596 updates\n",
      "2022-08-07 14:28:51 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 14:28:56 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 14:28:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt (epoch 1 @ 3596 updates, score 0.12) (writing took 6.579059072770178 seconds)\n",
      "2022-08-07 14:28:58 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2022-08-07 14:28:58 | INFO | train | epoch 001 | loss 13.161 | nll_loss 12.491 | ppl 5755.79 | wps 20580 | ups 6.18 | wpb 3329.2 | bsz 122 | num_updates 3596 | lr 8.99e-06 | gnorm 2.713 | loss_scale 16 | train_wall 545 | gb_free 32.5 | wall 583\n",
      "2022-08-07 14:28:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 002:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 14:28:58 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2022-08-07 14:28:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002: 100%|▉| 3598/3599 [09:15<00:00,  6.81it/s, loss=9.868, nll_loss=8.6742022-08-07 14:38:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:11,  1.03it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:09,  1.16it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.14it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:08,  1.09it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.00it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:06<00:07,  1.08s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:06,  1.16s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:06,  1.29s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.39s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:12<00:04,  1.54s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  85%|█████ | 11/13 [00:14<00:03,  1.74s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:17<00:02,  2.05s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|██████| 13/13 [00:20<00:00,  2.43s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 14:38:35 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.028 | nll_loss 8.849 | ppl 461.1 | bleu 0.89 | wps 1235.7 | wpb 1959.6 | bsz 89.7 | num_updates 7195 | best_bleu 0.89\n",
      "2022-08-07 14:38:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 7195 updates\n",
      "2022-08-07 14:38:35 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 14:38:39 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 14:38:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt (epoch 2 @ 7195 updates, score 0.89) (writing took 6.81056361598894 seconds)\n",
      "2022-08-07 14:38:42 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2022-08-07 14:38:42 | INFO | train | epoch 002 | loss 10.209 | nll_loss 9.054 | ppl 531.36 | wps 20531.1 | ups 6.17 | wpb 3329.2 | bsz 122.3 | num_updates 7195 | lr 7.45615e-06 | gnorm 1.905 | loss_scale 16 | train_wall 547 | gb_free 30.8 | wall 1166\n",
      "2022-08-07 14:38:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 003:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 14:38:42 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2022-08-07 14:38:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003: 100%|▉| 3598/3599 [09:09<00:00,  6.53it/s, loss=9.548, nll_loss=8.3162022-08-07 14:47:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:10,  1.15it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:09,  1.19it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.13it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:08,  1.08it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:08,  1.00s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:07,  1.07s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:06,  1.16s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:06,  1.29s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.39s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:12<00:04,  1.55s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  85%|█████ | 11/13 [00:14<00:03,  1.75s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:17<00:02,  2.06s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|██████| 13/13 [00:20<00:00,  2.44s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 14:48:12 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.69 | nll_loss 8.461 | ppl 352.4 | bleu 1.23 | wps 1228.2 | wpb 1959.6 | bsz 89.7 | num_updates 10794 | best_bleu 1.23\n",
      "2022-08-07 14:48:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 10794 updates\n",
      "2022-08-07 14:48:12 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 14:48:16 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 14:48:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt (epoch 3 @ 10794 updates, score 1.23) (writing took 6.948858272982761 seconds)\n",
      "2022-08-07 14:48:19 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2022-08-07 14:48:19 | INFO | train | epoch 003 | loss 9.688 | nll_loss 8.473 | ppl 355.27 | wps 20759.3 | ups 6.24 | wpb 3329.2 | bsz 122.3 | num_updates 10794 | lr 6.0875e-06 | gnorm 1.826 | loss_scale 16 | train_wall 541 | gb_free 30.8 | wall 1744\n",
      "2022-08-07 14:48:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 004:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 14:48:19 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2022-08-07 14:48:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004: 100%|▉| 3598/3599 [09:14<00:00,  6.78it/s, loss=9.336, nll_loss=8.0822022-08-07 14:57:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:   8%|▌      | 1/13 [00:01<00:12,  1.08s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:10,  1.08it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:09,  1.01it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:09,  1.00s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:05<00:08,  1.07s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:06<00:07,  1.14s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:07,  1.22s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:09<00:06,  1.34s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:11<00:05,  1.44s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:12<00:04,  1.58s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  85%|█████ | 11/13 [00:15<00:03,  1.78s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:18<00:02,  2.10s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|██████| 13/13 [00:21<00:00,  2.48s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 14:57:55 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.484 | nll_loss 8.233 | ppl 300.87 | bleu 1.43 | wps 1197.6 | wpb 1959.6 | bsz 89.7 | num_updates 14393 | best_bleu 1.43\n",
      "2022-08-07 14:57:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 14393 updates\n",
      "2022-08-07 14:57:55 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 14:58:00 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 14:58:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt (epoch 4 @ 14393 updates, score 1.43) (writing took 15.979991043917835 seconds)\n",
      "2022-08-07 14:58:11 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2022-08-07 14:58:11 | INFO | train | epoch 004 | loss 9.427 | nll_loss 8.181 | ppl 290.28 | wps 20235.8 | ups 6.08 | wpb 3329.2 | bsz 122.3 | num_updates 14393 | lr 5.27174e-06 | gnorm 1.794 | loss_scale 16 | train_wall 545 | gb_free 32.6 | wall 2336\n",
      "2022-08-07 14:58:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 005:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 14:58:11 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2022-08-07 14:58:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005:  72%|▋| 2599/3599 [06:38<02:39,  6.27it/s, loss=9.228, nll_loss=7.96,2022-08-07 15:04:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 005: 100%|▉| 3598/3599 [09:10<00:00,  7.08it/s, loss=9.171, nll_loss=7.8932022-08-07 15:07:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:   8%|▌      | 1/13 [00:01<00:12,  1.08s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:10,  1.08it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:09,  1.08it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:08,  1.05it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:08,  1.03s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:06<00:07,  1.09s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:07,  1.18s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:09<00:06,  1.30s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.39s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:12<00:04,  1.54s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  85%|█████ | 11/13 [00:14<00:03,  1.74s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:17<00:02,  2.05s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|██████| 13/13 [00:20<00:00,  2.43s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 15:07:43 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.343 | nll_loss 8.066 | ppl 268.03 | bleu 1.9 | wps 1230.8 | wpb 1959.6 | bsz 89.7 | num_updates 17991 | best_bleu 1.9\n",
      "2022-08-07 15:07:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 17991 updates\n",
      "2022-08-07 15:07:43 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 15:07:47 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 15:08:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt (epoch 5 @ 17991 updates, score 1.9) (writing took 19.054028399987146 seconds)\n",
      "2022-08-07 15:08:02 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2022-08-07 15:08:02 | INFO | train | epoch 005 | loss 9.256 | nll_loss 7.991 | ppl 254.35 | wps 20272.6 | ups 6.09 | wpb 3329 | bsz 121.8 | num_updates 17991 | lr 4.71522e-06 | gnorm 1.79 | loss_scale 16 | train_wall 542 | gb_free 32.1 | wall 2926\n",
      "2022-08-07 15:08:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 006:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 15:08:02 | INFO | fairseq.trainer | begin training epoch 6\n",
      "2022-08-07 15:08:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 006: 100%|▉| 3598/3599 [09:07<00:00,  6.59it/s, loss=9.039, nll_loss=7.75,2022-08-07 15:17:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:   8%|▌      | 1/13 [00:01<00:13,  1.13s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:10,  1.05it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:09,  1.06it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:08,  1.04it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:05<00:08,  1.04s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:06<00:07,  1.11s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:07,  1.19s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:09<00:06,  1.32s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.42s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:12<00:04,  1.56s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  85%|█████ | 11/13 [00:14<00:03,  1.76s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:17<00:02,  2.07s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|██████| 13/13 [00:21<00:00,  2.45s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 15:17:30 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.237 | nll_loss 7.945 | ppl 246.42 | bleu 1.92 | wps 1218.4 | wpb 1959.6 | bsz 89.7 | num_updates 21590 | best_bleu 1.92\n",
      "2022-08-07 15:17:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 21590 updates\n",
      "2022-08-07 15:17:30 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 15:17:35 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 15:17:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt (epoch 6 @ 21590 updates, score 1.92) (writing took 20.101238426053897 seconds)\n",
      "2022-08-07 15:17:51 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2022-08-07 15:17:51 | INFO | train | epoch 006 | loss 9.129 | nll_loss 7.849 | ppl 230.49 | wps 20346.2 | ups 6.11 | wpb 3329.2 | bsz 122.3 | num_updates 21590 | lr 4.30431e-06 | gnorm 1.796 | loss_scale 16 | train_wall 539 | gb_free 31 | wall 3515\n",
      "2022-08-07 15:17:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 007:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 15:17:51 | INFO | fairseq.trainer | begin training epoch 7\n",
      "2022-08-07 15:17:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 007: 100%|▉| 3598/3599 [09:08<00:00,  6.70it/s, loss=8.998, nll_loss=7.7022022-08-07 15:27:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:   8%|▌      | 1/13 [00:01<00:13,  1.14s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:10,  1.07it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:09,  1.09it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:08,  1.06it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:08,  1.02s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:06<00:07,  1.10s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:07,  1.18s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:09<00:06,  1.30s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.40s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:12<00:04,  1.55s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  85%|█████ | 11/13 [00:14<00:03,  1.75s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:17<00:02,  2.05s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|██████| 13/13 [00:20<00:00,  2.44s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 15:27:21 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 9.146 | nll_loss 7.843 | ppl 229.62 | bleu 2.15 | wps 1229.9 | wpb 1959.6 | bsz 89.7 | num_updates 25189 | best_bleu 2.15\n",
      "2022-08-07 15:27:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 25189 updates\n",
      "2022-08-07 15:27:21 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 15:27:25 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 15:27:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt (epoch 7 @ 25189 updates, score 2.15) (writing took 8.237784613855183 seconds)\n",
      "2022-08-07 15:27:29 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
      "2022-08-07 15:27:29 | INFO | train | epoch 007 | loss 9.028 | nll_loss 7.736 | ppl 213.19 | wps 20715.8 | ups 6.22 | wpb 3329.2 | bsz 122.3 | num_updates 25189 | lr 3.98497e-06 | gnorm 1.807 | loss_scale 16 | train_wall 540 | gb_free 31.3 | wall 4094\n",
      "2022-08-07 15:27:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 008:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 15:27:29 | INFO | fairseq.trainer | begin training epoch 8\n",
      "2022-08-07 15:27:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 008: 100%|▉| 3598/3599 [09:10<00:00,  6.57it/s, loss=9.012, nll_loss=7.72,2022-08-07 15:36:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:11,  1.06it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:09,  1.20it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.18it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:08,  1.11it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:08,  1.00s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:07,  1.09s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:07,  1.18s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:06,  1.31s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.41s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:12<00:04,  1.58s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  85%|█████ | 11/13 [00:14<00:03,  1.77s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:17<00:02,  2.07s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|██████| 13/13 [00:20<00:00,  2.45s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 15:37:00 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 9.089 | nll_loss 7.775 | ppl 219.01 | bleu 2.25 | wps 1224.1 | wpb 1959.6 | bsz 89.7 | num_updates 28788 | best_bleu 2.25\n",
      "2022-08-07 15:37:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 28788 updates\n",
      "2022-08-07 15:37:00 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 15:37:05 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 15:37:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt (epoch 8 @ 28788 updates, score 2.25) (writing took 7.222430350026116 seconds)\n",
      "2022-08-07 15:37:07 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
      "2022-08-07 15:37:07 | INFO | train | epoch 008 | loss 8.943 | nll_loss 7.642 | ppl 199.73 | wps 20713.4 | ups 6.22 | wpb 3329.2 | bsz 122.3 | num_updates 28788 | lr 3.72756e-06 | gnorm 1.803 | loss_scale 16 | train_wall 541 | gb_free 35.4 | wall 4672\n",
      "2022-08-07 15:37:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 009:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 15:37:08 | INFO | fairseq.trainer | begin training epoch 9\n",
      "2022-08-07 15:37:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 009: 100%|▉| 3598/3599 [09:06<00:00,  6.96it/s, loss=8.844, nll_loss=7.5322022-08-07 15:46:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:11,  1.00it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:09,  1.15it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.13it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:08,  1.08it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:08,  1.00s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:06<00:07,  1.09s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:07,  1.18s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:09<00:06,  1.31s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.40s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:12<00:04,  1.55s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  85%|█████ | 11/13 [00:14<00:03,  1.75s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:17<00:02,  2.06s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|██████| 13/13 [00:20<00:00,  2.44s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 15:46:36 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 9.015 | nll_loss 7.68 | ppl 205.14 | bleu 2.46 | wps 1228.8 | wpb 1959.6 | bsz 89.7 | num_updates 32387 | best_bleu 2.46\n",
      "2022-08-07 15:46:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 32387 updates\n",
      "2022-08-07 15:46:36 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 15:46:40 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 15:46:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt (epoch 9 @ 32387 updates, score 2.46) (writing took 7.211184049025178 seconds)\n",
      "2022-08-07 15:46:43 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
      "2022-08-07 15:46:43 | INFO | train | epoch 009 | loss 8.872 | nll_loss 7.562 | ppl 188.99 | wps 20826.5 | ups 6.26 | wpb 3329.2 | bsz 122.3 | num_updates 32387 | lr 3.51435e-06 | gnorm 1.812 | loss_scale 16 | train_wall 538 | gb_free 31.2 | wall 5247\n",
      "2022-08-07 15:46:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 010:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 15:46:43 | INFO | fairseq.trainer | begin training epoch 10\n",
      "2022-08-07 15:46:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 010:  87%|▊| 3146/3599 [07:58<01:09,  6.54it/s, loss=8.836, nll_loss=7.5222022-08-07 15:54:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 010: 100%|▉| 3598/3599 [09:06<00:00,  7.29it/s, loss=8.794, nll_loss=7.4752022-08-07 15:55:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:   8%|▌      | 1/13 [00:01<00:12,  1.07s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:10,  1.03it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:09,  1.05it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:08,  1.03it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:05<00:08,  1.05s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:06<00:07,  1.12s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:07,  1.20s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:09<00:06,  1.33s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.42s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:12<00:04,  1.57s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  85%|█████ | 11/13 [00:15<00:03,  1.77s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:17<00:02,  2.07s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|██████| 13/13 [00:21<00:00,  2.45s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 15:56:11 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.965 | nll_loss 7.634 | ppl 198.67 | bleu 2.37 | wps 1213.2 | wpb 1959.6 | bsz 89.7 | num_updates 35985 | best_bleu 2.46\n",
      "2022-08-07 15:56:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 35985 updates\n",
      "2022-08-07 15:56:11 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_last.pt\n",
      "2022-08-07 15:56:15 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_last.pt\n",
      "2022-08-07 15:56:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_last.pt (epoch 10 @ 35985 updates, score 2.37) (writing took 4.683846116997302 seconds)\n",
      "2022-08-07 15:56:15 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
      "2022-08-07 15:56:15 | INFO | train | epoch 010 | loss 8.809 | nll_loss 7.492 | ppl 179.97 | wps 20925 | ups 6.29 | wpb 3329.3 | bsz 122 | num_updates 35985 | lr 3.33403e-06 | gnorm 1.819 | loss_scale 16 | train_wall 538 | gb_free 33.5 | wall 5820\n",
      "2022-08-07 15:56:15 | INFO | fairseq_cli.train | done training in 5819.8 seconds\n",
      "2022-08-07 15:56:20 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard/kyoto_97//lr0.0005_drop0.2', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/kyoto_97/lr0.0005_drop0.2', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='kyoto/kyoto-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.2, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=10, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_src_tgt_embed=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/kyoto_97/lr0.0005_drop0.2', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir='tensorboard/kyoto_97//lr0.0005_drop0.2', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-07 15:56:21 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-07 15:56:21 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-07 15:56:25 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(146832, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(221864, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=221864, bias=False)\n",
      "  )\n",
      ")\n",
      "2022-08-07 15:56:25 | INFO | fairseq_cli.train | task: TranslationTask\n",
      "2022-08-07 15:56:25 | INFO | fairseq_cli.train | model: TransformerModel\n",
      "2022-08-07 15:56:25 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2022-08-07 15:56:25 | INFO | fairseq_cli.train | num. shared model params: 232,910,848 (num. trained: 232,910,848)\n",
      "2022-08-07 15:56:25 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2022-08-07 15:56:25 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: kyoto/kyoto-bin/valid.ja-en.ja\n",
      "2022-08-07 15:56:25 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: kyoto/kyoto-bin/valid.ja-en.en\n",
      "2022-08-07 15:56:25 | INFO | fairseq.tasks.translation | kyoto/kyoto-bin valid ja-en 1166 examples\n",
      "2022-08-07 15:56:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2022-08-07 15:56:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-08-07 15:56:30 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 47.544 GB ; name = NVIDIA RTX A6000                        \n",
      "2022-08-07 15:56:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-08-07 15:56:30 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2022-08-07 15:56:30 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
      "2022-08-07 15:56:30 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_last.pt\n",
      "2022-08-07 15:56:30 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_last.pt\n",
      "2022-08-07 15:56:30 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2022-08-07 15:56:30 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: kyoto/kyoto-bin/train.ja-en.ja\n",
      "2022-08-07 15:56:30 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: kyoto/kyoto-bin/train.ja-en.en\n",
      "2022-08-07 15:56:30 | INFO | fairseq.tasks.translation | kyoto/kyoto-bin train ja-en 440288 examples\n",
      "2022-08-07 15:56:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 001:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 15:56:30 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2022-08-07 15:56:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 001:   0%|                               | 1/3599 [00:00<58:24,  1.03it/s]2022-08-07 15:56:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 001:   2%|▌                             | 66/3599 [00:11<09:08,  6.44it/s]2022-08-07 15:56:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001:  16%|▏| 559/3599 [01:27<07:54,  6.40it/s, loss=10.993, nll_loss=9.9742022-08-07 15:57:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 001:  70%|▋| 2523/3599 [06:32<02:50,  6.31it/s, loss=8.617, nll_loss=7.2732022-08-07 16:03:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 001: 100%|▉| 3598/3599 [09:19<00:00,  6.71it/s, loss=8.302, nll_loss=6.9172022-08-07 16:05:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A2022-08-07 16:05:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-07 16:05:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-07 16:05:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   8%|▌      | 1/13 [00:01<00:17,  1.45s/it]\u001b[A2022-08-07 16:05:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-07 16:05:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-07 16:05:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:  15%|█      | 2/13 [00:02<00:12,  1.11s/it]\u001b[A2022-08-07 16:05:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-07 16:05:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-07 16:05:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:03<00:10,  1.06s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:04<00:09,  1.03s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:05<00:08,  1.06s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:06<00:07,  1.11s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:07,  1.17s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:09<00:06,  1.28s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.36s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:12<00:04,  1.53s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  85%|█████ | 11/13 [00:14<00:03,  1.68s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:17<00:01,  1.95s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|██████| 13/13 [00:20<00:00,  2.33s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 16:06:10 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.43 | nll_loss 7.035 | ppl 131.11 | bleu 2.2 | wps 1265.5 | wpb 1959.6 | bsz 89.7 | num_updates 3595\n",
      "2022-08-07 16:06:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 3595 updates\n",
      "2022-08-07 16:06:11 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 16:06:15 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 16:06:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_best.pt (epoch 1 @ 3595 updates, score 2.2) (writing took 7.092200767947361 seconds)\n",
      "2022-08-07 16:06:18 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2022-08-07 16:06:18 | INFO | train | epoch 001 | loss 9.659 | nll_loss 8.461 | ppl 352.46 | wps 20405.2 | ups 6.13 | wpb 3328.5 | bsz 121.3 | num_updates 3595 | lr 0.000449375 | gnorm 1.657 | loss_scale 8 | train_wall 550 | gb_free 32.5 | wall 587\n",
      "2022-08-07 16:06:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 002:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 16:06:18 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2022-08-07 16:06:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002: 100%|▉| 3598/3599 [09:10<00:00,  6.88it/s, loss=7.448, nll_loss=5.9632022-08-07 16:15:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:   8%|▌      | 1/13 [00:01<00:13,  1.16s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:10,  1.08it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.13it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:08,  1.12it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.07it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:07,  1.00s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:06,  1.09s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:06,  1.22s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.32s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.45s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  85%|█████ | 11/13 [00:13<00:03,  1.63s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:15<00:01,  1.74s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|██████| 13/13 [00:18<00:00,  1.93s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 16:15:47 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.735 | nll_loss 6.238 | ppl 75.48 | bleu 4.01 | wps 1421.8 | wpb 1959.6 | bsz 89.7 | num_updates 7194 | best_bleu 4.01\n",
      "2022-08-07 16:15:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 7194 updates\n",
      "2022-08-07 16:15:47 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 16:15:51 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 16:15:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_best.pt (epoch 2 @ 7194 updates, score 4.01) (writing took 6.8501405629795045 seconds)\n",
      "2022-08-07 16:15:54 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2022-08-07 16:15:54 | INFO | train | epoch 002 | loss 7.772 | nll_loss 6.325 | ppl 80.19 | wps 20799.1 | ups 6.25 | wpb 3329.2 | bsz 122.3 | num_updates 7194 | lr 0.000372833 | gnorm 1.166 | loss_scale 8 | train_wall 542 | gb_free 30.8 | wall 1164\n",
      "2022-08-07 16:15:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 003:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 16:15:54 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2022-08-07 16:15:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003:  85%|▊| 3076/3599 [07:52<01:19,  6.54it/s, loss=7.112, nll_loss=5.5872022-08-07 16:23:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0\n",
      "epoch 003: 100%|▉| 3598/3599 [09:12<00:00,  6.63it/s, loss=7.11, nll_loss=5.585,2022-08-07 16:25:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:11,  1.07it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:08,  1.27it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.24it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:07,  1.21it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.11it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:06,  1.02it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:06<00:06,  1.06s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:05,  1.17s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:09<00:05,  1.26s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.38s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  85%|█████ | 11/13 [00:13<00:03,  1.57s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:15<00:01,  1.84s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|██████| 13/13 [00:18<00:00,  2.22s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 16:25:25 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.505 | nll_loss 5.975 | ppl 62.89 | bleu 4.11 | wps 1364.2 | wpb 1959.6 | bsz 89.7 | num_updates 10792 | best_bleu 4.11\n",
      "2022-08-07 16:25:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 10792 updates\n",
      "2022-08-07 16:25:25 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 16:25:29 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 16:25:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_best.pt (epoch 3 @ 10792 updates, score 4.11) (writing took 15.061105252010748 seconds)\n",
      "2022-08-07 16:25:40 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2022-08-07 16:25:40 | INFO | train | epoch 003 | loss 7.218 | nll_loss 5.706 | ppl 52.19 | wps 20427.6 | ups 6.14 | wpb 3329 | bsz 121.8 | num_updates 10792 | lr 0.000304403 | gnorm 1.125 | loss_scale 4 | train_wall 543 | gb_free 30.8 | wall 1750\n",
      "2022-08-07 16:25:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 004:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 16:25:40 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2022-08-07 16:25:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004: 100%|▉| 3598/3599 [09:11<00:00,  6.68it/s, loss=6.882, nll_loss=5.3312022-08-07 16:34:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:10,  1.12it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:08,  1.31it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:07,  1.26it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:07,  1.18it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.08it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:07,  1.01s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:06<00:06,  1.10s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:06,  1.22s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:09<00:05,  1.31s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.45s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  85%|█████ | 11/13 [00:13<00:03,  1.62s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:16<00:01,  1.88s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|██████| 13/13 [00:18<00:00,  1.91s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 16:35:09 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.352 | nll_loss 5.805 | ppl 55.9 | bleu 4.83 | wps 1417.2 | wpb 1959.6 | bsz 89.7 | num_updates 14391 | best_bleu 4.83\n",
      "2022-08-07 16:35:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 14391 updates\n",
      "2022-08-07 16:35:09 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 16:35:14 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 16:35:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_best.pt (epoch 4 @ 14391 updates, score 4.83) (writing took 7.030655140057206 seconds)\n",
      "2022-08-07 16:35:16 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2022-08-07 16:35:16 | INFO | train | epoch 004 | loss 6.936 | nll_loss 5.39 | ppl 41.94 | wps 20784.5 | ups 6.24 | wpb 3329.2 | bsz 122.3 | num_updates 14391 | lr 0.000263606 | gnorm 1.168 | loss_scale 4 | train_wall 542 | gb_free 32.6 | wall 2326\n",
      "2022-08-07 16:35:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 005:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 16:35:17 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2022-08-07 16:35:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005: 100%|▉| 3598/3599 [09:11<00:00,  6.75it/s, loss=6.72, nll_loss=5.149,2022-08-07 16:44:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:   8%|▌      | 1/13 [00:01<00:12,  1.00s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:08,  1.26it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:07,  1.30it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:07,  1.24it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.14it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:06,  1.03it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:06<00:06,  1.06s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:07<00:05,  1.15s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:09<00:04,  1.20s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:10<00:04,  1.34s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  85%|█████ | 11/13 [00:12<00:02,  1.39s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:14<00:01,  1.55s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|██████| 13/13 [00:15<00:00,  1.52s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 16:44:44 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.36 | nll_loss 5.837 | ppl 57.17 | bleu 4.4 | wps 1640.5 | wpb 1959.6 | bsz 89.7 | num_updates 17990 | best_bleu 4.83\n",
      "2022-08-07 16:44:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 17990 updates\n",
      "2022-08-07 16:44:44 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_last.pt\n",
      "2022-08-07 16:44:49 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_last.pt\n",
      "2022-08-07 16:44:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_last.pt (epoch 5 @ 17990 updates, score 4.4) (writing took 4.714523756876588 seconds)\n",
      "2022-08-07 16:44:49 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2022-08-07 16:44:49 | INFO | train | epoch 005 | loss 6.751 | nll_loss 5.183 | ppl 36.34 | wps 20925.8 | ups 6.29 | wpb 3329.2 | bsz 122.3 | num_updates 17990 | lr 0.000235768 | gnorm 1.239 | loss_scale 4 | train_wall 542 | gb_free 32.1 | wall 2899\n",
      "2022-08-07 16:44:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 006:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 16:44:49 | INFO | fairseq.trainer | begin training epoch 6\n",
      "2022-08-07 16:44:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 006: 100%|▉| 3598/3599 [09:08<00:00,  6.60it/s, loss=6.539, nll_loss=4.9472022-08-07 16:53:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:11,  1.05it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:08,  1.24it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.24it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:07,  1.19it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.06it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:07,  1.01s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:06<00:06,  1.10s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:06,  1.21s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:09<00:05,  1.29s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.38s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  85%|█████ | 11/13 [00:13<00:03,  1.54s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:15<00:01,  1.60s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|██████| 13/13 [00:16<00:00,  1.65s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 16:54:15 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.296 | nll_loss 5.77 | ppl 54.56 | bleu 4.92 | wps 1536.6 | wpb 1959.6 | bsz 89.7 | num_updates 21589 | best_bleu 4.92\n",
      "2022-08-07 16:54:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 21589 updates\n",
      "2022-08-07 16:54:15 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 16:54:20 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 16:54:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_best.pt (epoch 6 @ 21589 updates, score 4.92) (writing took 18.570963846053928 seconds)\n",
      "2022-08-07 16:54:33 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2022-08-07 16:54:33 | INFO | train | epoch 006 | loss 6.609 | nll_loss 5.025 | ppl 32.57 | wps 20505.3 | ups 6.16 | wpb 3329.2 | bsz 122.3 | num_updates 21589 | lr 0.000215221 | gnorm 1.293 | loss_scale 4 | train_wall 540 | gb_free 31 | wall 3483\n",
      "2022-08-07 16:54:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 007:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 16:54:34 | INFO | fairseq.trainer | begin training epoch 7\n",
      "2022-08-07 16:54:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 007: 100%|▉| 3598/3599 [09:09<00:00,  6.69it/s, loss=6.512, nll_loss=4.9172022-08-07 17:03:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:11,  1.01it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:09,  1.21it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.19it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:07,  1.14it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.06it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:07,  1.02s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:06,  1.11s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:06,  1.24s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.32s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.46s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  85%|█████ | 11/13 [00:13<00:03,  1.65s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:16<00:01,  1.91s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|██████| 13/13 [00:19<00:00,  2.23s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 17:04:03 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.203 | nll_loss 5.636 | ppl 49.72 | bleu 5.41 | wps 1324.9 | wpb 1959.6 | bsz 89.7 | num_updates 25188 | best_bleu 5.41\n",
      "2022-08-07 17:04:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 25188 updates\n",
      "2022-08-07 17:04:03 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 17:04:07 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 17:04:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_best.pt (epoch 7 @ 25188 updates, score 5.41) (writing took 6.92473234096542 seconds)\n",
      "2022-08-07 17:04:09 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
      "2022-08-07 17:04:09 | INFO | train | epoch 007 | loss 6.498 | nll_loss 4.901 | ppl 29.87 | wps 20799.7 | ups 6.25 | wpb 3329.2 | bsz 122.3 | num_updates 25188 | lr 0.000199252 | gnorm 1.344 | loss_scale 4 | train_wall 541 | gb_free 31.3 | wall 4059\n",
      "2022-08-07 17:04:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 008:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 17:04:10 | INFO | fairseq.trainer | begin training epoch 8\n",
      "2022-08-07 17:04:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 008:  76%|▊| 2745/3599 [06:59<02:06,  6.76it/s, loss=6.434, nll_loss=4.8292022-08-07 17:11:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0\n",
      "epoch 008: 100%|▉| 3598/3599 [09:09<00:00,  6.60it/s, loss=6.561, nll_loss=4.9722022-08-07 17:13:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:10,  1.12it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:08,  1.32it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:07,  1.28it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:07,  1.19it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.08it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:07,  1.01s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:06<00:06,  1.10s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:06,  1.22s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:09<00:05,  1.31s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.44s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  85%|█████ | 11/13 [00:13<00:03,  1.60s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:15<00:01,  1.86s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|██████| 13/13 [00:18<00:00,  2.16s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 17:13:38 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.211 | nll_loss 5.678 | ppl 51.21 | bleu 5.28 | wps 1356.6 | wpb 1959.6 | bsz 89.7 | num_updates 28786 | best_bleu 5.41\n",
      "2022-08-07 17:13:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 28786 updates\n",
      "2022-08-07 17:13:38 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_last.pt\n",
      "2022-08-07 17:13:43 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_last.pt\n",
      "2022-08-07 17:13:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_last.pt (epoch 8 @ 28786 updates, score 5.28) (writing took 5.130695293890312 seconds)\n",
      "2022-08-07 17:13:43 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
      "2022-08-07 17:13:43 | INFO | train | epoch 008 | loss 6.406 | nll_loss 4.798 | ppl 27.81 | wps 20870.5 | ups 6.27 | wpb 3329.3 | bsz 122 | num_updates 28786 | lr 0.000186384 | gnorm 1.392 | loss_scale 4 | train_wall 541 | gb_free 35.4 | wall 4633\n",
      "2022-08-07 17:13:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 009:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 17:13:44 | INFO | fairseq.trainer | begin training epoch 9\n",
      "2022-08-07 17:13:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 009: 100%|▉| 3598/3599 [09:10<00:00,  6.89it/s, loss=6.346, nll_loss=4.7322022-08-07 17:22:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:11,  1.08it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:08,  1.30it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.19it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:07,  1.15it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.08it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:07,  1.01s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:06<00:06,  1.09s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:06,  1.21s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:09<00:05,  1.30s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.43s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  85%|█████ | 11/13 [00:13<00:03,  1.61s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:16<00:01,  1.90s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|██████| 13/13 [00:17<00:00,  1.78s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 17:23:12 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.264 | nll_loss 5.774 | ppl 54.73 | bleu 5.34 | wps 1453.9 | wpb 1959.6 | bsz 89.7 | num_updates 32385 | best_bleu 5.41\n",
      "2022-08-07 17:23:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 32385 updates\n",
      "2022-08-07 17:23:12 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_last.pt\n",
      "2022-08-07 17:23:17 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_last.pt\n",
      "2022-08-07 17:23:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_last.pt (epoch 9 @ 32385 updates, score 5.34) (writing took 5.07051317486912 seconds)\n",
      "2022-08-07 17:23:17 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
      "2022-08-07 17:23:17 | INFO | train | epoch 009 | loss 6.331 | nll_loss 4.714 | ppl 26.25 | wps 20878.2 | ups 6.27 | wpb 3329.2 | bsz 122.3 | num_updates 32385 | lr 0.000175723 | gnorm 1.442 | loss_scale 4 | train_wall 542 | gb_free 31.2 | wall 5207\n",
      "2022-08-07 17:23:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 010:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 17:23:17 | INFO | fairseq.trainer | begin training epoch 10\n",
      "2022-08-07 17:23:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 010: 100%|▉| 3598/3599 [09:13<00:00,  7.16it/s, loss=6.301, nll_loss=4.68,2022-08-07 17:32:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:10,  1.15it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:08,  1.31it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:07,  1.26it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:07,  1.20it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.11it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:06,  1.01it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:06<00:06,  1.07s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:05,  1.18s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:09<00:05,  1.26s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.35s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  85%|█████ | 11/13 [00:13<00:03,  1.55s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:15<00:01,  1.83s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|██████| 13/13 [00:18<00:00,  2.13s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 17:32:49 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.244 | nll_loss 5.728 | ppl 53.01 | bleu 5.18 | wps 1387.5 | wpb 1959.6 | bsz 89.7 | num_updates 35984 | best_bleu 5.41\n",
      "2022-08-07 17:32:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 35984 updates\n",
      "2022-08-07 17:32:49 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_last.pt\n",
      "2022-08-07 17:32:54 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_last.pt\n",
      "2022-08-07 17:32:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_last.pt (epoch 10 @ 35984 updates, score 5.18) (writing took 4.87292539793998 seconds)\n",
      "2022-08-07 17:32:54 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
      "2022-08-07 17:32:54 | INFO | train | epoch 010 | loss 6.266 | nll_loss 4.641 | ppl 24.96 | wps 20765.6 | ups 6.24 | wpb 3329.2 | bsz 122.3 | num_updates 35984 | lr 0.000166704 | gnorm 1.462 | loss_scale 4 | train_wall 544 | gb_free 33.5 | wall 5784\n",
      "2022-08-07 17:32:54 | INFO | fairseq_cli.train | done training in 5784.0 seconds\n",
      "2022-08-07 17:33:00 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard/kyoto_97//lr0.0005_drop0.3', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/kyoto_97/lr0.0005_drop0.3', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='kyoto/kyoto-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=10, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_src_tgt_embed=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/kyoto_97/lr0.0005_drop0.3', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir='tensorboard/kyoto_97//lr0.0005_drop0.3', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-07 17:33:00 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-07 17:33:00 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-07 17:33:04 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(146832, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(221864, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=221864, bias=False)\n",
      "  )\n",
      ")\n",
      "2022-08-07 17:33:04 | INFO | fairseq_cli.train | task: TranslationTask\n",
      "2022-08-07 17:33:04 | INFO | fairseq_cli.train | model: TransformerModel\n",
      "2022-08-07 17:33:04 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2022-08-07 17:33:04 | INFO | fairseq_cli.train | num. shared model params: 232,910,848 (num. trained: 232,910,848)\n",
      "2022-08-07 17:33:04 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2022-08-07 17:33:04 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: kyoto/kyoto-bin/valid.ja-en.ja\n",
      "2022-08-07 17:33:04 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: kyoto/kyoto-bin/valid.ja-en.en\n",
      "2022-08-07 17:33:04 | INFO | fairseq.tasks.translation | kyoto/kyoto-bin valid ja-en 1166 examples\n",
      "2022-08-07 17:33:09 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2022-08-07 17:33:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-08-07 17:33:09 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 47.544 GB ; name = NVIDIA RTX A6000                        \n",
      "2022-08-07 17:33:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-08-07 17:33:09 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2022-08-07 17:33:09 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
      "2022-08-07 17:33:09 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_last.pt\n",
      "2022-08-07 17:33:09 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_last.pt\n",
      "2022-08-07 17:33:09 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2022-08-07 17:33:09 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: kyoto/kyoto-bin/train.ja-en.ja\n",
      "2022-08-07 17:33:09 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: kyoto/kyoto-bin/train.ja-en.en\n",
      "2022-08-07 17:33:09 | INFO | fairseq.tasks.translation | kyoto/kyoto-bin train ja-en 440288 examples\n",
      "2022-08-07 17:33:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 001:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 17:33:09 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2022-08-07 17:33:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 001:   0%|                              | 13/3599 [00:02<10:06,  5.91it/s]2022-08-07 17:33:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 001:   2%|▌                             | 66/3599 [00:11<08:53,  6.62it/s]2022-08-07 17:33:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001:  16%|▏| 559/3599 [01:26<07:48,  6.49it/s, loss=11.093, nll_loss=10.082022-08-07 17:34:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 001:  70%|▋| 2523/3599 [06:26<02:46,  6.46it/s, loss=8.772, nll_loss=7.4472022-08-07 17:39:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 001: 100%|▉| 3598/3599 [09:10<00:00,  6.80it/s, loss=8.457, nll_loss=7.0912022-08-07 17:42:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A2022-08-07 17:42:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-07 17:42:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-07 17:42:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   8%|▌      | 1/13 [00:01<00:16,  1.37s/it]\u001b[A2022-08-07 17:42:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-07 17:42:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-07 17:42:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:  15%|█      | 2/13 [00:02<00:11,  1.08s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:03<00:10,  1.04s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:04<00:09,  1.02s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:05<00:08,  1.07s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:06<00:07,  1.12s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:07,  1.19s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:09<00:06,  1.29s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.37s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:12<00:04,  1.51s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  85%|█████ | 11/13 [00:14<00:03,  1.67s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:17<00:01,  1.95s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|██████| 13/13 [00:20<00:00,  2.25s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 17:42:40 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.568 | nll_loss 7.166 | ppl 143.59 | bleu 1.87 | wps 1278.7 | wpb 1959.6 | bsz 89.7 | num_updates 3595\n",
      "2022-08-07 17:42:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 3595 updates\n",
      "2022-08-07 17:42:40 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 17:42:45 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 17:42:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_best.pt (epoch 1 @ 3595 updates, score 1.87) (writing took 6.920852355193347 seconds)\n",
      "2022-08-07 17:42:47 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2022-08-07 17:42:47 | INFO | train | epoch 001 | loss 9.797 | nll_loss 8.615 | ppl 392.13 | wps 20735.8 | ups 6.23 | wpb 3328.3 | bsz 121.3 | num_updates 3595 | lr 0.000449375 | gnorm 1.59 | loss_scale 8 | train_wall 542 | gb_free 32.5 | wall 578\n",
      "2022-08-07 17:42:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 002:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 17:42:47 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2022-08-07 17:42:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002: 100%|▉| 3598/3599 [09:09<00:00,  6.85it/s, loss=7.627, nll_loss=6.1652022-08-07 17:51:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:11,  1.03it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:09,  1.22it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.22it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:07,  1.18it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.10it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:06,  1.02it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:06<00:06,  1.07s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:06,  1.20s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:09<00:05,  1.30s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.44s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  85%|█████ | 11/13 [00:13<00:03,  1.61s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:16<00:01,  1.91s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|██████| 13/13 [00:17<00:00,  1.72s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 17:52:15 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.939 | nll_loss 6.42 | ppl 85.6 | bleu 3.35 | wps 1480.8 | wpb 1959.6 | bsz 89.7 | num_updates 7194 | best_bleu 3.35\n",
      "2022-08-07 17:52:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 7194 updates\n",
      "2022-08-07 17:52:15 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 17:52:19 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 17:52:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_best.pt (epoch 2 @ 7194 updates, score 3.35) (writing took 8.15504692797549 seconds)\n",
      "2022-08-07 17:52:23 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2022-08-07 17:52:23 | INFO | train | epoch 002 | loss 7.942 | nll_loss 6.517 | ppl 91.55 | wps 20821.4 | ups 6.25 | wpb 3329.2 | bsz 122.3 | num_updates 7194 | lr 0.000372833 | gnorm 1.107 | loss_scale 8 | train_wall 541 | gb_free 30.8 | wall 1154\n",
      "2022-08-07 17:52:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 003:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 17:52:23 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2022-08-07 17:52:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003: 100%|▉| 3598/3599 [09:11<00:00,  6.54it/s, loss=7.341, nll_loss=5.8452022-08-07 18:01:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:10,  1.18it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:08,  1.33it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:07,  1.32it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:06,  1.29it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:06,  1.19it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:06,  1.10it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:06<00:06,  1.01s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:07<00:05,  1.12s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:09<00:04,  1.22s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:10<00:04,  1.34s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  85%|█████ | 11/13 [00:12<00:02,  1.43s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:14<00:01,  1.76s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|██████| 13/13 [00:16<00:00,  1.81s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 18:01:51 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.709 | nll_loss 6.186 | ppl 72.8 | bleu 2.97 | wps 1527 | wpb 1959.6 | bsz 89.7 | num_updates 10793 | best_bleu 3.35\n",
      "2022-08-07 18:01:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 10793 updates\n",
      "2022-08-07 18:01:51 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_last.pt\n",
      "2022-08-07 18:01:56 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_last.pt\n",
      "2022-08-07 18:01:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_last.pt (epoch 3 @ 10793 updates, score 2.97) (writing took 4.8027974418364465 seconds)\n",
      "2022-08-07 18:01:56 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2022-08-07 18:01:56 | INFO | train | epoch 003 | loss 7.44 | nll_loss 5.955 | ppl 62.04 | wps 20902 | ups 6.28 | wpb 3329.2 | bsz 122.3 | num_updates 10793 | lr 0.000304389 | gnorm 1.109 | loss_scale 8 | train_wall 543 | gb_free 30.8 | wall 1727\n",
      "2022-08-07 18:01:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 004:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 18:01:56 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2022-08-07 18:01:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004: 100%|▉| 3598/3599 [09:09<00:00,  6.59it/s, loss=7.147, nll_loss=5.63,2022-08-07 18:11:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:10,  1.10it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:08,  1.30it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:07,  1.28it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:07,  1.22it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.14it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:06,  1.04it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:06<00:06,  1.05s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:05,  1.17s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:09<00:05,  1.27s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.41s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  85%|█████ | 11/13 [00:13<00:03,  1.58s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:15<00:01,  1.87s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|██████| 13/13 [00:17<00:00,  1.80s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 18:11:23 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.569 | nll_loss 6.007 | ppl 64.3 | bleu 3.85 | wps 1478.4 | wpb 1959.6 | bsz 89.7 | num_updates 14392 | best_bleu 3.85\n",
      "2022-08-07 18:11:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 14392 updates\n",
      "2022-08-07 18:11:23 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 18:11:28 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 18:11:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_best.pt (epoch 4 @ 14392 updates, score 3.85) (writing took 7.269540241220966 seconds)\n",
      "2022-08-07 18:11:30 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2022-08-07 18:11:30 | INFO | train | epoch 004 | loss 7.195 | nll_loss 5.681 | ppl 51.32 | wps 20860.2 | ups 6.27 | wpb 3329.2 | bsz 122.3 | num_updates 14392 | lr 0.000263596 | gnorm 1.169 | loss_scale 8 | train_wall 541 | gb_free 32.6 | wall 2301\n",
      "2022-08-07 18:11:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 005:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 18:11:30 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2022-08-07 18:11:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005:  48%|▍| 1712/3599 [04:22<04:43,  6.66it/s, loss=7.042, nll_loss=5.5092022-08-07 18:15:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0\n",
      "epoch 005: 100%|▉| 3598/3599 [09:11<00:00,  7.16it/s, loss=6.997, nll_loss=5.46,2022-08-07 18:20:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:09,  1.26it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:07,  1.44it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:07,  1.42it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:02<00:06,  1.36it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:03<00:06,  1.27it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:04<00:06,  1.14it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:06<00:05,  1.02it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:07<00:05,  1.11s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:08<00:04,  1.22s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:10<00:03,  1.31s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  85%|█████ | 11/13 [00:12<00:03,  1.51s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:14<00:01,  1.79s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|██████| 13/13 [00:17<00:00,  2.13s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 18:21:00 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.52 | nll_loss 6.006 | ppl 64.26 | bleu 3.11 | wps 1437.4 | wpb 1959.6 | bsz 89.7 | num_updates 17990 | best_bleu 3.85\n",
      "2022-08-07 18:21:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 17990 updates\n",
      "2022-08-07 18:21:00 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_last.pt\n",
      "2022-08-07 18:21:04 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_last.pt\n",
      "2022-08-07 18:21:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_last.pt (epoch 5 @ 17990 updates, score 3.11) (writing took 4.69968571420759 seconds)\n",
      "2022-08-07 18:21:05 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2022-08-07 18:21:05 | INFO | train | epoch 005 | loss 7.037 | nll_loss 5.505 | ppl 45.41 | wps 20855.1 | ups 6.26 | wpb 3329 | bsz 121.8 | num_updates 17990 | lr 0.000235768 | gnorm 1.239 | loss_scale 4 | train_wall 543 | gb_free 32.1 | wall 2875\n",
      "2022-08-07 18:21:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 006:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 18:21:05 | INFO | fairseq.trainer | begin training epoch 6\n",
      "2022-08-07 18:21:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 006:  61%|▌| 2203/3599 [05:36<03:38,  6.40it/s, loss=6.856, nll_loss=5.3032022-08-07 18:26:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0\n",
      "epoch 006: 100%|▉| 3598/3599 [09:09<00:00,  6.57it/s, loss=6.849, nll_loss=5.2972022-08-07 18:30:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:10,  1.15it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:08,  1.32it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:07,  1.31it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:07,  1.25it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:03<00:06,  1.24it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:06,  1.11it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:06<00:05,  1.01it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:07<00:05,  1.13s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:09<00:04,  1.23s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:10<00:04,  1.37s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  85%|█████ | 11/13 [00:12<00:03,  1.57s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:15<00:01,  1.89s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|██████| 13/13 [00:18<00:00,  2.23s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 18:30:33 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.408 | nll_loss 5.881 | ppl 58.92 | bleu 4.05 | wps 1381.3 | wpb 1959.6 | bsz 89.7 | num_updates 21588 | best_bleu 4.05\n",
      "2022-08-07 18:30:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 21588 updates\n",
      "2022-08-07 18:30:33 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 18:30:38 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 18:30:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_best.pt (epoch 6 @ 21588 updates, score 4.05) (writing took 19.843436161987484 seconds)\n",
      "2022-08-07 18:30:53 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2022-08-07 18:30:53 | INFO | train | epoch 006 | loss 6.915 | nll_loss 5.369 | ppl 41.33 | wps 20351.5 | ups 6.11 | wpb 3329 | bsz 121.8 | num_updates 21588 | lr 0.000215226 | gnorm 1.286 | loss_scale 2 | train_wall 541 | gb_free 31 | wall 3464\n",
      "2022-08-07 18:30:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 007:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 18:30:53 | INFO | fairseq.trainer | begin training epoch 7\n",
      "2022-08-07 18:30:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 007: 100%|▉| 3598/3599 [09:09<00:00,  6.70it/s, loss=6.829, nll_loss=5.2742022-08-07 18:40:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:11,  1.04it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:08,  1.25it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.24it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:07,  1.21it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.12it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:06,  1.03it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:06<00:06,  1.07s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:05,  1.20s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:09<00:05,  1.29s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.44s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  85%|█████ | 11/13 [00:13<00:03,  1.65s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:16<00:01,  1.96s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|██████| 13/13 [00:19<00:00,  2.36s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 18:40:23 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.33 | nll_loss 5.764 | ppl 54.35 | bleu 4.95 | wps 1308.2 | wpb 1959.6 | bsz 89.7 | num_updates 25187 | best_bleu 4.95\n",
      "2022-08-07 18:40:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 25187 updates\n",
      "2022-08-07 18:40:23 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 18:40:27 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 18:40:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_best.pt (epoch 7 @ 25187 updates, score 4.95) (writing took 7.106273872079328 seconds)\n",
      "2022-08-07 18:40:30 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
      "2022-08-07 18:40:30 | INFO | train | epoch 007 | loss 6.825 | nll_loss 5.268 | ppl 38.54 | wps 20770.9 | ups 6.24 | wpb 3329.2 | bsz 122.3 | num_updates 25187 | lr 0.000199256 | gnorm 1.334 | loss_scale 2 | train_wall 541 | gb_free 31.3 | wall 4041\n",
      "2022-08-07 18:40:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 008:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 18:40:30 | INFO | fairseq.trainer | begin training epoch 8\n",
      "2022-08-07 18:40:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 008: 100%|▉| 3598/3599 [09:09<00:00,  6.55it/s, loss=6.883, nll_loss=5.3352022-08-07 18:49:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:09,  1.22it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:07,  1.40it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:07,  1.35it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:07,  1.27it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:06,  1.16it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:06,  1.06it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:06<00:06,  1.03s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:07<00:05,  1.16s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:09<00:05,  1.27s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.40s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  85%|█████ | 11/13 [00:13<00:03,  1.59s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:15<00:01,  1.87s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|██████| 13/13 [00:18<00:00,  2.12s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 18:49:58 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.306 | nll_loss 5.776 | ppl 54.8 | bleu 4.67 | wps 1395.6 | wpb 1959.6 | bsz 89.7 | num_updates 28786 | best_bleu 4.95\n",
      "2022-08-07 18:49:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 28786 updates\n",
      "2022-08-07 18:49:58 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_last.pt\n",
      "2022-08-07 18:50:02 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_last.pt\n",
      "2022-08-07 18:50:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_last.pt (epoch 8 @ 28786 updates, score 4.67) (writing took 4.843943122075871 seconds)\n",
      "2022-08-07 18:50:02 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
      "2022-08-07 18:50:02 | INFO | train | epoch 008 | loss 6.748 | nll_loss 5.183 | ppl 36.34 | wps 20932 | ups 6.29 | wpb 3329.2 | bsz 122.3 | num_updates 28786 | lr 0.000186384 | gnorm 1.379 | loss_scale 2 | train_wall 540 | gb_free 35.4 | wall 4613\n",
      "2022-08-07 18:50:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 009:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 18:50:03 | INFO | fairseq.trainer | begin training epoch 9\n",
      "2022-08-07 18:50:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 009: 100%|▉| 3598/3599 [09:09<00:00,  6.97it/s, loss=6.696, nll_loss=5.1272022-08-07 18:59:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:09,  1.30it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:06,  1.57it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:06,  1.48it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:02<00:06,  1.36it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:03<00:06,  1.22it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:04<00:06,  1.10it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:06<00:06,  1.01s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:07<00:05,  1.13s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:09<00:04,  1.24s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:10<00:04,  1.40s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  85%|█████ | 11/13 [00:12<00:03,  1.59s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:15<00:01,  1.89s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|██████| 13/13 [00:18<00:00,  2.26s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 18:59:31 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.347 | nll_loss 5.831 | ppl 56.93 | bleu 4.08 | wps 1373.8 | wpb 1959.6 | bsz 89.7 | num_updates 32385 | best_bleu 4.95\n",
      "2022-08-07 18:59:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 32385 updates\n",
      "2022-08-07 18:59:31 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_last.pt\n",
      "2022-08-07 18:59:35 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_last.pt\n",
      "2022-08-07 18:59:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_last.pt (epoch 9 @ 32385 updates, score 4.08) (writing took 4.908729544142261 seconds)\n",
      "2022-08-07 18:59:36 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
      "2022-08-07 18:59:36 | INFO | train | epoch 009 | loss 6.684 | nll_loss 5.112 | ppl 34.59 | wps 20903.8 | ups 6.28 | wpb 3329.2 | bsz 122.3 | num_updates 32385 | lr 0.000175723 | gnorm 1.44 | loss_scale 2 | train_wall 541 | gb_free 31.2 | wall 5186\n",
      "2022-08-07 18:59:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 010:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 18:59:36 | INFO | fairseq.trainer | begin training epoch 10\n",
      "2022-08-07 18:59:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 010: 100%|▉| 3598/3599 [09:09<00:00,  7.17it/s, loss=6.659, nll_loss=5.0852022-08-07 19:08:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:10,  1.15it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:09,  1.21it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.22it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:07,  1.20it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.14it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:06,  1.05it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:06<00:06,  1.03s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:07<00:05,  1.15s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:09<00:04,  1.25s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.39s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  85%|█████ | 11/13 [00:13<00:03,  1.58s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:15<00:01,  1.88s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|██████| 13/13 [00:18<00:00,  2.28s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 19:09:04 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.278 | nll_loss 5.767 | ppl 54.46 | bleu 4.57 | wps 1346.8 | wpb 1959.6 | bsz 89.7 | num_updates 35984 | best_bleu 4.95\n",
      "2022-08-07 19:09:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 35984 updates\n",
      "2022-08-07 19:09:04 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_last.pt\n",
      "2022-08-07 19:09:09 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_last.pt\n",
      "2022-08-07 19:09:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_last.pt (epoch 10 @ 35984 updates, score 4.57) (writing took 4.992245111847296 seconds)\n",
      "2022-08-07 19:09:09 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
      "2022-08-07 19:09:09 | INFO | train | epoch 010 | loss 6.629 | nll_loss 5.051 | ppl 33.15 | wps 20897.1 | ups 6.28 | wpb 3329.2 | bsz 122.3 | num_updates 35984 | lr 0.000166704 | gnorm 1.465 | loss_scale 2 | train_wall 541 | gb_free 33.5 | wall 5760\n",
      "2022-08-07 19:09:09 | INFO | fairseq_cli.train | done training in 5759.7 seconds\n",
      "2022-08-07 19:09:14 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard/kyoto_97//lr0.0001_drop0.2', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/kyoto_97/lr0.0001_drop0.2', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='kyoto/kyoto-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.2, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0001], lr_scheduler='inverse_sqrt', max_epoch=10, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_src_tgt_embed=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/kyoto_97/lr0.0001_drop0.2', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir='tensorboard/kyoto_97//lr0.0001_drop0.2', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-07 19:09:15 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-07 19:09:15 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-07 19:09:19 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(146832, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(221864, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=221864, bias=False)\n",
      "  )\n",
      ")\n",
      "2022-08-07 19:09:19 | INFO | fairseq_cli.train | task: TranslationTask\n",
      "2022-08-07 19:09:19 | INFO | fairseq_cli.train | model: TransformerModel\n",
      "2022-08-07 19:09:19 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2022-08-07 19:09:19 | INFO | fairseq_cli.train | num. shared model params: 232,910,848 (num. trained: 232,910,848)\n",
      "2022-08-07 19:09:19 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2022-08-07 19:09:19 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: kyoto/kyoto-bin/valid.ja-en.ja\n",
      "2022-08-07 19:09:19 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: kyoto/kyoto-bin/valid.ja-en.en\n",
      "2022-08-07 19:09:19 | INFO | fairseq.tasks.translation | kyoto/kyoto-bin valid ja-en 1166 examples\n",
      "2022-08-07 19:09:22 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2022-08-07 19:09:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-08-07 19:09:22 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 47.544 GB ; name = NVIDIA RTX A6000                        \n",
      "2022-08-07 19:09:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-08-07 19:09:22 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2022-08-07 19:09:22 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
      "2022-08-07 19:09:22 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_last.pt\n",
      "2022-08-07 19:09:22 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_last.pt\n",
      "2022-08-07 19:09:22 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2022-08-07 19:09:22 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: kyoto/kyoto-bin/train.ja-en.ja\n",
      "2022-08-07 19:09:22 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: kyoto/kyoto-bin/train.ja-en.en\n",
      "2022-08-07 19:09:22 | INFO | fairseq.tasks.translation | kyoto/kyoto-bin train ja-en 440288 examples\n",
      "2022-08-07 19:09:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 001:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 19:09:23 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2022-08-07 19:09:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 001:   0%|                               | 1/3599 [00:00<59:32,  1.01it/s]2022-08-07 19:09:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 001:   2%|▌                             | 66/3599 [00:11<09:03,  6.50it/s]2022-08-07 19:09:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001:  10%| | 359/3599 [00:56<08:15,  6.54it/s, loss=14.954, nll_loss=14.552022-08-07 19:10:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 001: 100%|▉| 3598/3599 [09:11<00:00,  6.68it/s, loss=9.02, nll_loss=7.729,2022-08-07 19:18:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A2022-08-07 19:18:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-07 19:18:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-07 19:18:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   8%|▌      | 1/13 [00:01<00:15,  1.27s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  15%|█      | 2/13 [00:02<00:11,  1.03s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:03<00:10,  1.00s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:04<00:09,  1.01s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:05<00:08,  1.08s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:06<00:07,  1.14s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:07,  1.22s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:09<00:06,  1.34s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:11<00:05,  1.43s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:13<00:04,  1.60s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  85%|█████ | 11/13 [00:15<00:03,  1.79s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:18<00:02,  2.08s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|██████| 13/13 [00:21<00:00,  2.45s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 19:18:56 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.149 | nll_loss 7.86 | ppl 232.27 | bleu 1.72 | wps 1206.7 | wpb 1959.6 | bsz 89.7 | num_updates 3596\n",
      "2022-08-07 19:18:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 3596 updates\n",
      "2022-08-07 19:18:56 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 19:19:01 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 19:19:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt (epoch 1 @ 3596 updates, score 1.72) (writing took 6.87424317910336 seconds)\n",
      "2022-08-07 19:19:03 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2022-08-07 19:19:03 | INFO | train | epoch 001 | loss 10.743 | nll_loss 9.7 | ppl 831.93 | wps 20652.4 | ups 6.2 | wpb 3329 | bsz 122 | num_updates 3596 | lr 8.99e-05 | gnorm 2.151 | loss_scale 16 | train_wall 543 | gb_free 32.5 | wall 581\n",
      "2022-08-07 19:19:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 002:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 19:19:03 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2022-08-07 19:19:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002: 100%|▉| 3598/3599 [09:11<00:00,  6.68it/s, loss=8.016, nll_loss=6.6072022-08-07 19:28:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:   8%|▌      | 1/13 [00:01<00:12,  1.07s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:10,  1.09it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:09,  1.11it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:08,  1.09it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.02it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:07,  1.06s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:06,  1.14s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:06,  1.27s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.36s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:12<00:04,  1.50s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  85%|█████ | 11/13 [00:14<00:03,  1.67s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:16<00:01,  1.97s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|██████| 13/13 [00:20<00:00,  2.36s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 19:28:35 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.197 | nll_loss 6.781 | ppl 110 | bleu 4.23 | wps 1272.5 | wpb 1959.6 | bsz 89.7 | num_updates 7195 | best_bleu 4.23\n",
      "2022-08-07 19:28:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 7195 updates\n",
      "2022-08-07 19:28:35 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 19:28:39 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 19:28:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt (epoch 2 @ 7195 updates, score 4.23) (writing took 7.993770779110491 seconds)\n",
      "2022-08-07 19:28:43 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2022-08-07 19:28:43 | INFO | train | epoch 002 | loss 8.391 | nll_loss 7.026 | ppl 130.36 | wps 20659.8 | ups 6.21 | wpb 3329.2 | bsz 122.3 | num_updates 7195 | lr 7.45615e-05 | gnorm 1.488 | loss_scale 16 | train_wall 543 | gb_free 30.8 | wall 1161\n",
      "2022-08-07 19:28:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 003:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 19:28:43 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2022-08-07 19:28:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003:  85%|▊| 3076/3599 [07:51<01:19,  6.54it/s, loss=7.62, nll_loss=6.165,2022-08-07 19:36:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 003: 100%|▉| 3598/3599 [09:11<00:00,  6.57it/s, loss=7.609, nll_loss=6.1522022-08-07 19:37:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:10,  1.13it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:08,  1.24it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.18it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:07,  1.13it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.05it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:07,  1.03s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:06,  1.11s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:06,  1.24s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.33s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.48s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  85%|█████ | 11/13 [00:13<00:03,  1.66s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:16<00:01,  1.95s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|██████| 13/13 [00:19<00:00,  2.33s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 19:38:14 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.766 | nll_loss 6.31 | ppl 79.33 | bleu 5.48 | wps 1290.6 | wpb 1959.6 | bsz 89.7 | num_updates 10793 | best_bleu 5.48\n",
      "2022-08-07 19:38:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 10793 updates\n",
      "2022-08-07 19:38:14 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 19:38:19 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 19:38:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt (epoch 3 @ 10793 updates, score 5.48) (writing took 11.785882859956473 seconds)\n",
      "2022-08-07 19:38:26 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2022-08-07 19:38:26 | INFO | train | epoch 003 | loss 7.771 | nll_loss 6.333 | ppl 80.62 | wps 20541.1 | ups 6.17 | wpb 3329 | bsz 121.8 | num_updates 10793 | lr 6.08778e-05 | gnorm 1.427 | loss_scale 8 | train_wall 543 | gb_free 30.8 | wall 1744\n",
      "2022-08-07 19:38:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 004:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 19:38:26 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2022-08-07 19:38:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004: 100%|▉| 3598/3599 [09:09<00:00,  6.70it/s, loss=7.305, nll_loss=5.8122022-08-07 19:47:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:11,  1.02it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:09,  1.19it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.15it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:08,  1.09it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.01it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:07,  1.07s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:06,  1.15s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:06,  1.27s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.36s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:12<00:04,  1.50s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  85%|█████ | 11/13 [00:14<00:03,  1.69s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:16<00:01,  1.98s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|██████| 13/13 [00:20<00:00,  2.36s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 19:47:56 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.476 | nll_loss 5.943 | ppl 61.54 | bleu 6.97 | wps 1267.8 | wpb 1959.6 | bsz 89.7 | num_updates 14392 | best_bleu 6.97\n",
      "2022-08-07 19:47:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 14392 updates\n",
      "2022-08-07 19:47:56 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 19:48:01 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 19:48:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt (epoch 4 @ 14392 updates, score 6.97) (writing took 8.06402043113485 seconds)\n",
      "2022-08-07 19:48:04 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2022-08-07 19:48:04 | INFO | train | epoch 004 | loss 7.411 | nll_loss 5.929 | ppl 60.93 | wps 20721.9 | ups 6.22 | wpb 3329.2 | bsz 122.3 | num_updates 14392 | lr 5.27193e-05 | gnorm 1.475 | loss_scale 8 | train_wall 541 | gb_free 32.6 | wall 2322\n",
      "2022-08-07 19:48:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 005:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 19:48:04 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2022-08-07 19:48:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005: 100%|▉| 3598/3599 [09:11<00:00,  6.95it/s, loss=7.058, nll_loss=5.5322022-08-07 19:57:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:09,  1.26it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:07,  1.38it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:07,  1.30it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:07,  1.21it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.10it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:07,  1.03s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:06<00:06,  1.11s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:06,  1.23s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:09<00:05,  1.32s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.46s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  85%|█████ | 11/13 [00:13<00:03,  1.59s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:15<00:01,  1.88s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|██████| 13/13 [00:18<00:00,  1.97s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 19:57:34 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.265 | nll_loss 5.738 | ppl 53.37 | bleu 9.17 | wps 1400.5 | wpb 1959.6 | bsz 89.7 | num_updates 17991 | best_bleu 9.17\n",
      "2022-08-07 19:57:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 17991 updates\n",
      "2022-08-07 19:57:34 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 19:57:38 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 19:57:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt (epoch 5 @ 17991 updates, score 9.17) (writing took 6.9725760200526565 seconds)\n",
      "2022-08-07 19:57:41 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2022-08-07 19:57:41 | INFO | train | epoch 005 | loss 7.143 | nll_loss 5.628 | ppl 49.45 | wps 20779.5 | ups 6.24 | wpb 3329.2 | bsz 122.3 | num_updates 17991 | lr 4.71522e-05 | gnorm 1.535 | loss_scale 8 | train_wall 543 | gb_free 32.1 | wall 2899\n",
      "2022-08-07 19:57:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 006:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 19:57:41 | INFO | fairseq.trainer | begin training epoch 6\n",
      "2022-08-07 19:57:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 006: 100%|▉| 3598/3599 [09:11<00:00,  6.60it/s, loss=6.79, nll_loss=5.233,2022-08-07 20:06:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:10,  1.13it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:08,  1.27it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.22it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:07,  1.16it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.06it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:07,  1.02s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:06<00:06,  1.11s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:06,  1.23s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:09<00:05,  1.33s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.47s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  85%|█████ | 11/13 [00:13<00:03,  1.65s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:16<00:01,  1.94s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|██████| 13/13 [00:19<00:00,  2.32s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 20:07:13 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.058 | nll_loss 5.49 | ppl 44.93 | bleu 10.12 | wps 1299.1 | wpb 1959.6 | bsz 89.7 | num_updates 21590 | best_bleu 10.12\n",
      "2022-08-07 20:07:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 21590 updates\n",
      "2022-08-07 20:07:13 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 20:07:17 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 20:07:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt (epoch 6 @ 21590 updates, score 10.12) (writing took 7.00920339114964 seconds)\n",
      "2022-08-07 20:07:20 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2022-08-07 20:07:20 | INFO | train | epoch 006 | loss 6.925 | nll_loss 5.383 | ppl 41.74 | wps 20697.7 | ups 6.22 | wpb 3329.2 | bsz 122.3 | num_updates 21590 | lr 4.30431e-05 | gnorm 1.585 | loss_scale 8 | train_wall 543 | gb_free 31 | wall 3477\n",
      "2022-08-07 20:07:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 007:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 20:07:20 | INFO | fairseq.trainer | begin training epoch 7\n",
      "2022-08-07 20:07:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 007: 100%|▉| 3598/3599 [09:15<00:00,  6.55it/s, loss=6.714, nll_loss=5.1462022-08-07 20:16:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:10,  1.10it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:08,  1.24it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.19it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:07,  1.13it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.04it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:07,  1.04s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:06,  1.12s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:06,  1.24s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.34s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.48s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  85%|█████ | 11/13 [00:13<00:03,  1.67s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:16<00:01,  1.96s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|██████| 13/13 [00:19<00:00,  2.34s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 20:16:56 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.926 | nll_loss 5.304 | ppl 39.52 | bleu 10.56 | wps 1286.7 | wpb 1959.6 | bsz 89.7 | num_updates 25189 | best_bleu 10.56\n",
      "2022-08-07 20:16:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 25189 updates\n",
      "2022-08-07 20:16:56 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 20:17:00 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 20:17:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt (epoch 7 @ 25189 updates, score 10.56) (writing took 14.751303929137066 seconds)\n",
      "2022-08-07 20:17:11 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
      "2022-08-07 20:17:11 | INFO | train | epoch 007 | loss 6.747 | nll_loss 5.183 | ppl 36.33 | wps 20276.4 | ups 6.09 | wpb 3329.2 | bsz 122.3 | num_updates 25189 | lr 3.98497e-05 | gnorm 1.629 | loss_scale 8 | train_wall 547 | gb_free 31.3 | wall 4068\n",
      "2022-08-07 20:17:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 008:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 20:17:11 | INFO | fairseq.trainer | begin training epoch 8\n",
      "2022-08-07 20:17:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 008:  76%|▊| 2745/3599 [07:06<02:09,  6.58it/s, loss=6.578, nll_loss=4.9922022-08-07 20:24:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 008: 100%|▉| 3598/3599 [09:18<00:00,  6.43it/s, loss=6.673, nll_loss=5.1, 2022-08-07 20:26:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:10,  1.15it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:09,  1.22it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.18it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:08,  1.12it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.03it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:07,  1.05s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:06,  1.14s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:06,  1.25s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.34s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.48s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  85%|█████ | 11/13 [00:14<00:03,  1.65s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:16<00:01,  1.84s/it]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|██████| 13/13 [00:18<00:00,  2.03s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 20:26:48 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.806 | nll_loss 5.205 | ppl 36.89 | bleu 11.77 | wps 1360.4 | wpb 1959.6 | bsz 89.7 | num_updates 28787 | best_bleu 11.77\n",
      "2022-08-07 20:26:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 28787 updates\n",
      "2022-08-07 20:26:48 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 20:26:53 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 20:27:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt (epoch 8 @ 28787 updates, score 11.77) (writing took 17.3775147870183 seconds)\n",
      "2022-08-07 20:27:06 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
      "2022-08-07 20:27:06 | INFO | train | epoch 008 | loss 6.596 | nll_loss 5.014 | ppl 32.31 | wps 20129 | ups 6.05 | wpb 3329.3 | bsz 122 | num_updates 28787 | lr 3.72762e-05 | gnorm 1.666 | loss_scale 8 | train_wall 549 | gb_free 35.4 | wall 4664\n",
      "2022-08-07 20:27:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 009:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 20:27:06 | INFO | fairseq.trainer | begin training epoch 9\n",
      "2022-08-07 20:27:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 009: 100%|▉| 3598/3599 [09:16<00:00,  6.85it/s, loss=6.393, nll_loss=4.7862022-08-07 20:36:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:10,  1.19it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:08,  1.35it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:07,  1.28it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:07,  1.21it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.11it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:06,  1.02it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:06<00:06,  1.07s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:05,  1.18s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:09<00:05,  1.28s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.42s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  85%|█████ | 11/13 [00:13<00:03,  1.60s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:15<00:01,  1.76s/it]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|██████| 13/13 [00:17<00:00,  1.94s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 20:36:40 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.72 | nll_loss 5.079 | ppl 33.8 | bleu 13.06 | wps 1434.8 | wpb 1959.6 | bsz 89.7 | num_updates 32386 | best_bleu 13.06\n",
      "2022-08-07 20:36:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 32386 updates\n",
      "2022-08-07 20:36:40 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 20:36:45 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 20:36:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt (epoch 9 @ 32386 updates, score 13.06) (writing took 6.9510048110969365 seconds)\n",
      "2022-08-07 20:36:47 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
      "2022-08-07 20:36:47 | INFO | train | epoch 009 | loss 6.469 | nll_loss 4.87 | ppl 29.25 | wps 20611.4 | ups 6.19 | wpb 3329.2 | bsz 122.3 | num_updates 32386 | lr 3.5144e-05 | gnorm 1.702 | loss_scale 8 | train_wall 547 | gb_free 31.2 | wall 5245\n",
      "2022-08-07 20:36:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 010:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 20:36:47 | INFO | fairseq.trainer | begin training epoch 10\n",
      "2022-08-07 20:36:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 010: 100%|▉| 3598/3599 [09:12<00:00,  7.21it/s, loss=6.334, nll_loss=4.7192022-08-07 20:46:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:10,  1.12it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:08,  1.27it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.22it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:07,  1.16it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.07it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:06,  1.03it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:06<00:06,  1.06s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:05,  1.19s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:09<00:05,  1.28s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.42s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  85%|█████ | 11/13 [00:13<00:03,  1.51s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:15<00:01,  1.73s/it]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|██████| 13/13 [00:17<00:00,  1.95s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 20:46:18 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.594 | nll_loss 4.967 | ppl 31.28 | bleu 13.66 | wps 1435.7 | wpb 1959.6 | bsz 89.7 | num_updates 35985 | best_bleu 13.66\n",
      "2022-08-07 20:46:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 35985 updates\n",
      "2022-08-07 20:46:18 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 20:46:22 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt\n",
      "2022-08-07 20:46:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt (epoch 10 @ 35985 updates, score 13.66) (writing took 8.60859303502366 seconds)\n",
      "2022-08-07 20:46:26 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
      "2022-08-07 20:46:26 | INFO | train | epoch 010 | loss 6.357 | nll_loss 4.744 | ppl 26.81 | wps 20687.5 | ups 6.21 | wpb 3329.2 | bsz 122.3 | num_updates 35985 | lr 3.33403e-05 | gnorm 1.731 | loss_scale 8 | train_wall 543 | gb_free 33.5 | wall 5824\n",
      "2022-08-07 20:46:26 | INFO | fairseq_cli.train | done training in 5823.9 seconds\n",
      "2022-08-07 20:46:32 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard/kyoto_97//lr0.0001_drop0.3', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/kyoto_97/lr0.0001_drop0.3', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='kyoto/kyoto-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0001], lr_scheduler='inverse_sqrt', max_epoch=10, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_src_tgt_embed=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/kyoto_97/lr0.0001_drop0.3', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir='tensorboard/kyoto_97//lr0.0001_drop0.3', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-07 20:46:32 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-07 20:46:32 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-07 20:46:36 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(146832, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(221864, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=221864, bias=False)\n",
      "  )\n",
      ")\n",
      "2022-08-07 20:46:36 | INFO | fairseq_cli.train | task: TranslationTask\n",
      "2022-08-07 20:46:36 | INFO | fairseq_cli.train | model: TransformerModel\n",
      "2022-08-07 20:46:36 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2022-08-07 20:46:36 | INFO | fairseq_cli.train | num. shared model params: 232,910,848 (num. trained: 232,910,848)\n",
      "2022-08-07 20:46:36 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2022-08-07 20:46:36 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: kyoto/kyoto-bin/valid.ja-en.ja\n",
      "2022-08-07 20:46:36 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: kyoto/kyoto-bin/valid.ja-en.en\n",
      "2022-08-07 20:46:36 | INFO | fairseq.tasks.translation | kyoto/kyoto-bin valid ja-en 1166 examples\n",
      "2022-08-07 20:46:40 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2022-08-07 20:46:40 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-08-07 20:46:40 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 47.544 GB ; name = NVIDIA RTX A6000                        \n",
      "2022-08-07 20:46:40 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-08-07 20:46:40 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2022-08-07 20:46:40 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
      "2022-08-07 20:46:40 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_last.pt\n",
      "2022-08-07 20:46:40 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_last.pt\n",
      "2022-08-07 20:46:40 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2022-08-07 20:46:40 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: kyoto/kyoto-bin/train.ja-en.ja\n",
      "2022-08-07 20:46:40 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: kyoto/kyoto-bin/train.ja-en.en\n",
      "2022-08-07 20:46:40 | INFO | fairseq.tasks.translation | kyoto/kyoto-bin train ja-en 440288 examples\n",
      "2022-08-07 20:46:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 001:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 20:46:40 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2022-08-07 20:46:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 001:   1%|▏                             | 21/3599 [00:04<09:19,  6.40it/s]2022-08-07 20:46:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 001:   2%|▌                             | 66/3599 [00:11<09:03,  6.50it/s]2022-08-07 20:46:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001:  10%| | 359/3599 [00:56<08:22,  6.45it/s, loss=15.039, nll_loss=14.652022-08-07 20:47:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 001:  70%|▋| 2523/3599 [06:32<02:50,  6.32it/s, loss=9.634, nll_loss=8.4152022-08-07 20:53:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 001: 100%|▉| 3598/3599 [09:19<00:00,  6.55it/s, loss=9.172, nll_loss=7.8992022-08-07 20:56:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A2022-08-07 20:56:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-07 20:56:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-07 20:56:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   8%|▌      | 1/13 [00:01<00:15,  1.32s/it]\u001b[A2022-08-07 20:56:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
      "2022-08-07 20:56:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2022-08-07 20:56:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:  15%|█      | 2/13 [00:02<00:11,  1.08s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:03<00:10,  1.04s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:04<00:09,  1.04s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:05<00:08,  1.10s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:06<00:08,  1.15s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:08<00:07,  1.23s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:09<00:06,  1.35s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:11<00:05,  1.44s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:13<00:04,  1.60s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  85%|█████ | 11/13 [00:15<00:03,  1.78s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:18<00:02,  2.08s/it]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|██████| 13/13 [00:21<00:00,  2.45s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 20:56:21 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.286 | nll_loss 8.007 | ppl 257.29 | bleu 1.54 | wps 1201 | wpb 1959.6 | bsz 89.7 | num_updates 3595\n",
      "2022-08-07 20:56:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 3595 updates\n",
      "2022-08-07 20:56:21 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 20:56:26 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 20:56:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_best.pt (epoch 1 @ 3595 updates, score 1.54) (writing took 6.5339429769665 seconds)\n",
      "2022-08-07 20:56:28 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2022-08-07 20:56:28 | INFO | train | epoch 001 | loss 10.877 | nll_loss 9.848 | ppl 921.88 | wps 20397.6 | ups 6.13 | wpb 3328.7 | bsz 121.4 | num_updates 3595 | lr 8.9875e-05 | gnorm 2.047 | loss_scale 8 | train_wall 550 | gb_free 32.5 | wall 588\n",
      "2022-08-07 20:56:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 002:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 20:56:28 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2022-08-07 20:56:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002: 100%|▉| 3598/3599 [09:14<00:00,  6.78it/s, loss=8.185, nll_loss=6.7962022-08-07 21:05:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:   8%|▌      | 1/13 [00:01<00:12,  1.02s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:09,  1.14it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.14it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:08,  1.10it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.02it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:07,  1.06s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:06,  1.15s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:06,  1.27s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.37s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:12<00:04,  1.52s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  85%|█████ | 11/13 [00:14<00:03,  1.70s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:16<00:01,  1.98s/it]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|██████| 13/13 [00:20<00:00,  2.36s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 21:06:02 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.377 | nll_loss 6.96 | ppl 124.5 | bleu 3.73 | wps 1266.5 | wpb 1959.6 | bsz 89.7 | num_updates 7194 | best_bleu 3.73\n",
      "2022-08-07 21:06:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 7194 updates\n",
      "2022-08-07 21:06:02 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 21:06:07 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 21:06:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_best.pt (epoch 2 @ 7194 updates, score 3.73) (writing took 6.880972575163469 seconds)\n",
      "2022-08-07 21:06:09 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2022-08-07 21:06:09 | INFO | train | epoch 002 | loss 8.567 | nll_loss 7.223 | ppl 149.45 | wps 20605.2 | ups 6.19 | wpb 3329.2 | bsz 122.3 | num_updates 7194 | lr 7.45667e-05 | gnorm 1.442 | loss_scale 8 | train_wall 545 | gb_free 30.8 | wall 1169\n",
      "2022-08-07 21:06:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 003:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 21:06:09 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2022-08-07 21:06:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003: 100%|▉| 3598/3599 [09:15<00:00,  6.44it/s, loss=7.833, nll_loss=6.4032022-08-07 21:15:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:10,  1.12it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:09,  1.20it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.17it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:08,  1.12it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.04it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:07,  1.03s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:06,  1.12s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:06,  1.25s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.34s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.49s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  85%|█████ | 11/13 [00:14<00:03,  1.67s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:16<00:01,  1.97s/it]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|██████| 13/13 [00:19<00:00,  2.36s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 21:15:45 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.985 | nll_loss 6.531 | ppl 92.46 | bleu 4.36 | wps 1275.7 | wpb 1959.6 | bsz 89.7 | num_updates 10793 | best_bleu 4.36\n",
      "2022-08-07 21:15:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 10793 updates\n",
      "2022-08-07 21:15:45 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 21:15:49 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 21:15:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_best.pt (epoch 3 @ 10793 updates, score 4.36) (writing took 6.877248396864161 seconds)\n",
      "2022-08-07 21:15:52 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2022-08-07 21:15:52 | INFO | train | epoch 003 | loss 7.975 | nll_loss 6.561 | ppl 94.44 | wps 20568.7 | ups 6.18 | wpb 3329.2 | bsz 122.3 | num_updates 10793 | lr 6.08778e-05 | gnorm 1.385 | loss_scale 8 | train_wall 546 | gb_free 30.8 | wall 1752\n",
      "2022-08-07 21:15:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 004:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 21:15:52 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2022-08-07 21:15:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004: 100%|▉| 3598/3599 [09:12<00:00,  6.49it/s, loss=7.596, nll_loss=6.1382022-08-07 21:25:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:11,  1.01it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:09,  1.16it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.12it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:08,  1.07it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:08,  1.01s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:06<00:07,  1.09s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:07,  1.17s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:06,  1.30s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.39s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:12<00:04,  1.53s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  85%|█████ | 11/13 [00:14<00:03,  1.72s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:17<00:02,  2.01s/it]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|██████| 13/13 [00:20<00:00,  2.38s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 21:25:25 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.77 | nll_loss 6.267 | ppl 77.02 | bleu 4.73 | wps 1247.9 | wpb 1959.6 | bsz 89.7 | num_updates 14392 | best_bleu 4.73\n",
      "2022-08-07 21:25:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 14392 updates\n",
      "2022-08-07 21:25:25 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 21:25:30 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 21:25:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_best.pt (epoch 4 @ 14392 updates, score 4.73) (writing took 6.858712367946282 seconds)\n",
      "2022-08-07 21:25:32 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2022-08-07 21:25:32 | INFO | train | epoch 004 | loss 7.681 | nll_loss 6.232 | ppl 75.16 | wps 20650.8 | ups 6.2 | wpb 3329.2 | bsz 122.3 | num_updates 14392 | lr 5.27193e-05 | gnorm 1.416 | loss_scale 8 | train_wall 544 | gb_free 32.6 | wall 2332\n",
      "2022-08-07 21:25:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 005:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 21:25:32 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2022-08-07 21:25:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005: 100%|▉| 3598/3599 [09:11<00:00,  6.89it/s, loss=7.4, nll_loss=5.915, 2022-08-07 21:34:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:09,  1.26it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:07,  1.40it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:07,  1.29it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:07,  1.20it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.08it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:07,  1.02s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:06<00:06,  1.12s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:06,  1.25s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.37s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.51s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  85%|█████ | 11/13 [00:13<00:03,  1.67s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:16<00:01,  1.97s/it]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|██████| 13/13 [00:18<00:00,  2.08s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 21:35:02 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.588 | nll_loss 6.097 | ppl 68.46 | bleu 6.09 | wps 1347.3 | wpb 1959.6 | bsz 89.7 | num_updates 17991 | best_bleu 6.09\n",
      "2022-08-07 21:35:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 17991 updates\n",
      "2022-08-07 21:35:02 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 21:35:07 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 21:35:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_best.pt (epoch 5 @ 17991 updates, score 6.09) (writing took 7.849005185998976 seconds)\n",
      "2022-08-07 21:35:10 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2022-08-07 21:35:10 | INFO | train | epoch 005 | loss 7.473 | nll_loss 5.999 | ppl 63.94 | wps 20724.6 | ups 6.23 | wpb 3329.2 | bsz 122.3 | num_updates 17991 | lr 4.71522e-05 | gnorm 1.454 | loss_scale 8 | train_wall 542 | gb_free 32.1 | wall 2910\n",
      "2022-08-07 21:35:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 006:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 21:35:10 | INFO | fairseq.trainer | begin training epoch 6\n",
      "2022-08-07 21:35:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 006:  61%|▌| 2203/3599 [05:39<03:37,  6.42it/s, loss=7.218, nll_loss=5.7122022-08-07 21:40:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 006: 100%|▉| 3598/3599 [09:12<00:00,  6.62it/s, loss=7.166, nll_loss=5.6542022-08-07 21:44:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:10,  1.12it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:08,  1.26it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.21it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:07,  1.15it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.06it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:07,  1.04s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:06,  1.12s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:06,  1.25s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.34s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.48s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  85%|█████ | 11/13 [00:13<00:03,  1.68s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:16<00:01,  1.98s/it]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|██████| 13/13 [00:19<00:00,  2.37s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 21:44:43 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.403 | nll_loss 5.881 | ppl 58.92 | bleu 7.13 | wps 1278.1 | wpb 1959.6 | bsz 89.7 | num_updates 21589 | best_bleu 7.13\n",
      "2022-08-07 21:44:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 21589 updates\n",
      "2022-08-07 21:44:43 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 21:44:47 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 21:44:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_best.pt (epoch 6 @ 21589 updates, score 7.13) (writing took 6.803178153000772 seconds)\n",
      "2022-08-07 21:44:49 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2022-08-07 21:44:49 | INFO | train | epoch 006 | loss 7.293 | nll_loss 5.796 | ppl 55.57 | wps 20678.4 | ups 6.21 | wpb 3329 | bsz 121.8 | num_updates 21589 | lr 4.30441e-05 | gnorm 1.49 | loss_scale 8 | train_wall 543 | gb_free 31 | wall 3489\n",
      "2022-08-07 21:44:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 007:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 21:44:50 | INFO | fairseq.trainer | begin training epoch 7\n",
      "2022-08-07 21:44:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 007: 100%|▉| 3598/3599 [09:11<00:00,  6.53it/s, loss=7.109, nll_loss=5.5892022-08-07 21:54:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:   8%|▌      | 1/13 [00:00<00:10,  1.10it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  15%|█      | 2/13 [00:01<00:08,  1.24it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  23%|█▌     | 3/13 [00:02<00:08,  1.19it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:03<00:07,  1.13it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:04<00:07,  1.04it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  46%|███▏   | 6/13 [00:05<00:07,  1.04s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:07<00:06,  1.13s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:08<00:06,  1.26s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  69%|████▊  | 9/13 [00:10<00:05,  1.35s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:11<00:04,  1.49s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  85%|█████ | 11/13 [00:14<00:03,  1.69s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  92%|█████▌| 12/13 [00:16<00:01,  1.99s/it]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|██████| 13/13 [00:20<00:00,  2.38s/it]\u001b[A\n",
      "                                                                                \u001b[A2022-08-07 21:54:21 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.282 | nll_loss 5.702 | ppl 52.05 | bleu 7.85 | wps 1270.2 | wpb 1959.6 | bsz 89.7 | num_updates 25188 | best_bleu 7.85\n",
      "2022-08-07 21:54:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 25188 updates\n",
      "2022-08-07 21:54:21 | INFO | fairseq.trainer | Saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 21:54:26 | INFO | fairseq.trainer | Finished saving checkpoint to /workspace/NLP100knock/checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_best.pt\n",
      "2022-08-07 21:54:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_best.pt (epoch 7 @ 25188 updates, score 7.85) (writing took 8.003343746066093 seconds)\n",
      "2022-08-07 21:54:29 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
      "2022-08-07 21:54:29 | INFO | train | epoch 007 | loss 7.139 | nll_loss 5.623 | ppl 49.28 | wps 20661.1 | ups 6.21 | wpb 3329.2 | bsz 122.3 | num_updates 25188 | lr 3.98504e-05 | gnorm 1.532 | loss_scale 8 | train_wall 543 | gb_free 31.3 | wall 4069\n",
      "2022-08-07 21:54:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3599\n",
      "epoch 008:   0%|                                       | 0/3599 [00:00<?, ?it/s]2022-08-07 21:54:30 | INFO | fairseq.trainer | begin training epoch 8\n",
      "2022-08-07 21:54:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 008:  53%|▌| 1914/3599 [04:57<04:15,  6.60it/s, loss=6.978, nll_loss=5.442"
     ]
    }
   ],
   "source": [
    "for l in [1e-5, 5e-4, 1e-4]:\n",
    "    for d in [0.2, 0.3]:\n",
    "        cmd = f\"\"\"CUDA_VISIBLE_DEVICES=2 fairseq-train kyoto/kyoto-bin \\\n",
    "            --save-dir checkpoints/kyoto_97/lr{l}_drop{d} \\\n",
    "            --tensorboard-logdir tensorboard/kyoto_97//lr{l}_drop{d} \\\n",
    "            --arch transformer --task translation \\\n",
    "            --share-decoder-input-output-embed \\\n",
    "            --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "            --lr {l} --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "            --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "            --dropout {d} --weight-decay 0.0001 \\\n",
    "        \"\"\"+\\\n",
    "        \"\"\" --max-tokens 4096 \\\n",
    "            --eval-bleu \\\n",
    "            --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
    "            --eval-bleu-detok space \\\n",
    "            --eval-bleu-remove-bpe \\\n",
    "            --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
    "            --fp16 \\\n",
    "            --max-epoch 10 \\\n",
    "            --no-epoch-checkpoints\n",
    "        \"\"\"\n",
    "\n",
    "        !$cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-10 05:43:16 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto_97/lr1e-05_drop0.2/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-10 05:43:17 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-10 05:43:17 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-10 05:43:17 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto_97/lr1e-05_drop0.2/checkpoint_best.pt\n",
      "2022-08-10 05:43:27 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-10 05:43:27 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-10 05:51:54 | INFO | fairseq_cli.interactive | Total time: 517.966 seconds; translation time: 501.395\n",
      "2022-08-10 05:51:59 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-10 05:52:00 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-10 05:52:00 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-10 05:52:00 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto_97/lr1e-05_drop0.3/checkpoint_best.pt\n",
      "2022-08-10 05:52:10 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-10 05:52:10 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-10 06:01:41 | INFO | fairseq_cli.interactive | Total time: 581.599 seconds; translation time: 564.947\n",
      "2022-08-10 06:01:45 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-10 06:01:46 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-10 06:01:46 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-10 06:01:46 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto_97/lr0.0005_drop0.2/checkpoint_best.pt\n",
      "2022-08-10 06:01:56 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-10 06:01:56 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-10 06:08:35 | INFO | fairseq_cli.interactive | Total time: 410.209 seconds; translation time: 393.721\n",
      "2022-08-10 06:08:40 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-10 06:08:41 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-10 06:08:41 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-10 06:08:41 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto_97/lr0.0005_drop0.3/checkpoint_best.pt\n",
      "2022-08-10 06:08:51 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-10 06:08:51 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-10 06:15:11 | INFO | fairseq_cli.interactive | Total time: 390.812 seconds; translation time: 374.150\n",
      "2022-08-10 06:15:15 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-10 06:15:16 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-10 06:15:16 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-10 06:15:16 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto_97/lr0.0001_drop0.2/checkpoint_best.pt\n",
      "2022-08-10 06:15:27 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-10 06:15:27 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-10 06:21:41 | INFO | fairseq_cli.interactive | Total time: 385.795 seconds; translation time: 368.642\n",
      "2022-08-10 06:21:46 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'kyoto/kyoto-test.ja'}, 'model': None, 'task': {'_name': 'translation', 'data': 'kyoto/kyoto-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-10 06:21:46 | INFO | fairseq.tasks.translation | [ja] dictionary: 146832 types\n",
      "2022-08-10 06:21:46 | INFO | fairseq.tasks.translation | [en] dictionary: 221864 types\n",
      "2022-08-10 06:21:46 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/kyoto_97/lr0.0001_drop0.3/checkpoint_best.pt\n",
      "2022-08-10 06:21:57 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-10 06:21:57 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
      "2022-08-10 06:28:59 | INFO | fairseq_cli.interactive | Total time: 433.674 seconds; translation time: 416.733\n"
     ]
    }
   ],
   "source": [
    "for l in [1e-5, 5e-4, 1e-4]:\n",
    "    for d in [0.2, 0.3]:\n",
    "        !CUDA_VISIBLE_DEVICES=2 fairseq-interactive kyoto/kyoto-bin \\\n",
    "            --path checkpoints/kyoto_97/lr{l}_drop{d}/checkpoint_best.pt  \\\n",
    "            --input kyoto/kyoto-test.ja \\\n",
    "            --task translation \\\n",
    "            | grep '^H' | cut -f3 | sed -r 's/(@@ )|(@@ ?$)//g' > 97/lr{l}_drop{d}.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結果表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:1e-05\t dropout:0.2\n",
      "Namespace(ignore_case=False, order=4, ref='kyoto/kyoto-test.en', sacrebleu=False, sentence_bleu=False, sys='97/lr1e-05_drop0.2.out')\n",
      "BLEU4 = 3.19, 23.1/5.2/1.5/0.6 (BP=1.000, ratio=1.228, syslen=32833, reflen=26734)\n",
      "lr:1e-05\t dropout:0.3\n",
      "Namespace(ignore_case=False, order=4, ref='kyoto/kyoto-test.en', sacrebleu=False, sentence_bleu=False, sys='97/lr1e-05_drop0.3.out')\n",
      "BLEU4 = 2.24, 19.1/3.9/1.0/0.3 (BP=1.000, ratio=1.402, syslen=37483, reflen=26734)\n",
      "lr:0.0005\t dropout:0.2\n",
      "Namespace(ignore_case=False, order=4, ref='kyoto/kyoto-test.en', sacrebleu=False, sentence_bleu=False, sys='97/lr0.0005_drop0.2.out')\n",
      "BLEU4 = 6.60, 32.6/9.2/3.7/1.8 (BP=0.990, ratio=0.990, syslen=26475, reflen=26734)\n",
      "lr:0.0005\t dropout:0.3\n",
      "Namespace(ignore_case=False, order=4, ref='kyoto/kyoto-test.en', sacrebleu=False, sentence_bleu=False, sys='97/lr0.0005_drop0.3.out')\n",
      "BLEU4 = 5.67, 30.5/8.4/3.3/1.6 (BP=0.939, ratio=0.941, syslen=25158, reflen=26734)\n",
      "lr:0.0001\t dropout:0.2\n",
      "Namespace(ignore_case=False, order=4, ref='kyoto/kyoto-test.en', sacrebleu=False, sentence_bleu=False, sys='97/lr0.0001_drop0.2.out')\n",
      "BLEU4 = 16.32, 47.9/21.5/11.6/6.9 (BP=0.962, ratio=0.962, syslen=25727, reflen=26734)\n",
      "lr:0.0001\t dropout:0.3\n",
      "Namespace(ignore_case=False, order=4, ref='kyoto/kyoto-test.en', sacrebleu=False, sentence_bleu=False, sys='97/lr0.0001_drop0.3.out')\n",
      "BLEU4 = 11.75, 38.9/15.4/7.6/4.2 (BP=1.000, ratio=1.060, syslen=28327, reflen=26734)\n"
     ]
    }
   ],
   "source": [
    "for l in [1e-5, 5e-4, 1e-4]:\n",
    "    for d in [0.2, 0.3]:\n",
    "        print(f'lr:{l},dropout:{d}')\n",
    "        !fairseq-score --sys 97/lr{l}_drop{d}.out --ref kyoto/kyoto-test.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 98. ドメイン適応\n",
    "Japanese-English Subtitle Corpus (JESC)やJParaCrawlなどの翻訳データを活用し，KFTTのテストデータの性能向上を試みよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-07 01:44:40--  http://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/release/3.0/bitext/en-ja.tar.gz\n",
      "Resolving www.kecl.ntt.co.jp (www.kecl.ntt.co.jp)... 163.137.218.58\n",
      "Connecting to www.kecl.ntt.co.jp (www.kecl.ntt.co.jp)|163.137.218.58|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2972770796 (2.8G) [application/x-gzip]\n",
      "Saving to: ‘en-ja.tar.gz’\n",
      "\n",
      "en-ja.tar.gz        100%[===================>]   2.77G  6.00MB/s    in 11m 3s  \n",
      "\n",
      "2022-08-07 01:55:43 (4.28 MB/s) - ‘en-ja.tar.gz’ saved [2972770796/2972770796]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/release/3.0/bitext/en-ja.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en-ja/\n",
      "en-ja/LICENSE\n",
      "en-ja/CITATION\n",
      "en-ja/en-ja.bicleaner05.txt\n"
     ]
    }
   ],
   "source": [
    "!tar zxvf en-ja.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001vip.cocolog-nifty.com\t0001vip.cocolog-nifty.com\t0.535\tAnd everyone will not care that it is not you.\t鼻・口のところはあらかじめ少し切っておくといいですね。\n",
      "0001vip.cocolog-nifty.com\t0001vip.cocolog-nifty.com\t0.557\tAnd everyone will not care that it is not you.\tアドレス置いとくので、消されないうちにメールくれたら嬉しいです。\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 en-ja/en-ja.bicleaner05.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en-ja/en-ja.bicleaner05.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "data = text.splitlines()\n",
    "data = [x.split('\\t') for x in data]\n",
    "data = [x for x in data if len(x) == 5]\n",
    "source = [x[0] for x in data]\n",
    "target = [x[1] for x in data]\n",
    "\n",
    "with open('jparacrawl/jparacrawl.ja', 'w') as f, open('jparacrawl/jparacrawl.en', 'w') as g:\n",
    "    for j, e in zip(source, target):\n",
    "        print(j, file=f)\n",
    "        print(e, file=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = [x[0] for x in data]\n",
    "target = [x[1] for x in data]\n",
    "source_train, source_dev, target_train, target_dev = train_test_split(source, target, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "source_train, source_dev, target_train, target_dev = train_test_split(source, target, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=jparacrawl/jparacrawl.ja --model_prefix=jparacrawl_ja --vocab_size=16000 --character_coverage=1.0\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: jparacrawl/jparacrawl.ja\n",
      "  input_format: \n",
      "  model_prefix: jparacrawl_ja\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 16000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: jparacrawl/jparacrawl.ja\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 2000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 3000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 4000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 5000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 6000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 7000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 8000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 9000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 10000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 11000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 12000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 13000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 14000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 15000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 16000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 17000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 18000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 19000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 20000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 21000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 22000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 23000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 24000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 25000000 lines\n",
      "trainer_interface.cc(117) LOG(WARNING) Too many sentences are loaded! (25740064), which may slow down training.\n",
      "trainer_interface.cc(119) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(122) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 25740064 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=1722703219\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 100% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=12346\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 25740064 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 1000000 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 25740064\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 32682602\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 32682602 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=870603 obj=73.442 num_tokens=371935070 num_tokens/piece=427.215\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=732925 obj=64.3487 num_tokens=373957912 num_tokens/piece=510.227\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=549664 obj=64.1611 num_tokens=375942517 num_tokens/piece=683.95\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=549379 obj=64.0884 num_tokens=377060935 num_tokens/piece=686.34\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=412033 obj=64.1242 num_tokens=378277693 num_tokens/piece=918.076\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=412033 obj=64.1049 num_tokens=378716830 num_tokens/piece=919.142\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=309024 obj=64.2528 num_tokens=381803214 num_tokens/piece=1235.51\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=309024 obj=64.1861 num_tokens=382016126 num_tokens/piece=1236.2\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=231768 obj=64.5959 num_tokens=389198041 num_tokens/piece=1679.26\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=231767 obj=64.4244 num_tokens=389420329 num_tokens/piece=1680.22\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=173824 obj=65.1824 num_tokens=400430738 num_tokens/piece=2303.66\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=173822 obj=64.903 num_tokens=400669863 num_tokens/piece=2305.06\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=130365 obj=65.9624 num_tokens=414055452 num_tokens/piece=3176.12\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=130362 obj=65.6218 num_tokens=414374174 num_tokens/piece=3178.64\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=97771 obj=66.9707 num_tokens=429441678 num_tokens/piece=4392.32\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=97771 obj=66.5911 num_tokens=429623563 num_tokens/piece=4394.18\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=73328 obj=68.2466 num_tokens=447111262 num_tokens/piece=6097.42\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=73328 obj=67.8262 num_tokens=447362994 num_tokens/piece=6100.85\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=54996 obj=69.8645 num_tokens=467500302 num_tokens/piece=8500.62\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=54996 obj=69.3895 num_tokens=467683030 num_tokens/piece=8503.95\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=41247 obj=71.8876 num_tokens=492155507 num_tokens/piece=11931.9\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=41246 obj=71.322 num_tokens=492335529 num_tokens/piece=11936.6\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=30934 obj=74.5397 num_tokens=523218869 num_tokens/piece=16914\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=30934 obj=73.8302 num_tokens=523283556 num_tokens/piece=16916.1\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=23200 obj=78.4159 num_tokens=566148236 num_tokens/piece=24402.9\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=23200 obj=77.4243 num_tokens=566276602 num_tokens/piece=24408.5\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=17600 obj=84.6448 num_tokens=634066659 num_tokens/piece=36026.5\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=17600 obj=83.0258 num_tokens=634143999 num_tokens/piece=36030.9\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: jparacrawl_ja.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: jparacrawl_ja.vocab\n"
     ]
    }
   ],
   "source": [
    "# 日本語のサブワード化\n",
    "import sentencepiece as spm\n",
    "import re\n",
    "\n",
    "spm.SentencePieceTrainer.Train('--input=jparacrawl/jparacrawl.ja --model_prefix=jparacrawl_ja --vocab_size=16000 --character_coverage=1.0')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('jparacrawl_ja.model')\n",
    "\n",
    "\n",
    "with open('jparacrawl/jparacrawl.ja', 'r') as f, open('jparacrawl/jparacrawl_sub.ja', 'w') as g:\n",
    "    for x in f:\n",
    "        x = x.strip()\n",
    "        x = re.sub(r'\\s+', ' ', x)\n",
    "        x = sp.encode_as_pieces(x)\n",
    "        x = ' '.join(x)\n",
    "        print(x, file=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日本語のサブワード化\n",
    "import sentencepiece as spm\n",
    "import re\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('jparacrawl_ja.model')\n",
    "\n",
    "\n",
    "with open('jparacrawl/jparacrawl.ja', 'r') as f, open('jparacrawl/jparacrawl_sub.ja', 'w') as g:\n",
    "    for x in f:\n",
    "        x = x.strip()\n",
    "        x = re.sub(r'\\s+', ' ', x)\n",
    "        x = sp.encode_as_pieces(x)\n",
    "        x = ' '.join(x)\n",
    "        print(x, file=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|#####################################| 16000/16000 [12:50<00:00, 20.76it/s]\n"
     ]
    }
   ],
   "source": [
    "!subword-nmt learn-bpe -s 16000 < jparacrawl/jparacrawl.en > jparacrawl/jparacrawl_en.codes\n",
    "!subword-nmt apply-bpe -c jparacrawl/jparacrawl_en.codes < jparacrawl/jparacrawl.en > jparacrawl/jparacrawl_sub.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('jparacrawl/jparacrawl_train.ja', 'w') as jt,  open('jparacrawl/jparacrawl_dev.ja', 'w') as jd,\\\n",
    "     open('jparacrawl/jparacrawl_train.en', 'w') as et,  open('jparacrawl/jparacrawl_dev.en', 'w') as ed:\n",
    "    for train_j, train_e in zip(source_train, target_train):\n",
    "        print(train_j, file=jt)\n",
    "        print(train_e, file=et)\n",
    "    for dev_j, dev_e in zip(source_dev, target_dev):\n",
    "        print(dev_j,file=jd)\n",
    "        print(dev_e, file=ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "with open('jparacrawl/jparacrawl_sub.ja', 'r') as j,  open('jparacrawl/jparacrawl.en', 'r') as e:\n",
    "    source = j.read().splitlines()\n",
    "    target = e.read().splitlines()\n",
    "    source_train, source_dev, target_train, target_dev = train_test_split(source, target, test_size=0.3)\n",
    "with open('jparacrawl/jparacrawl_train.ja', 'w') as jt,  open('jparacrawl/jparacrawl_dev.ja', 'w') as jd,\\\n",
    "     open('jparacrawl/jparacrawl_train.en', 'w') as et,  open('jparacrawl/jparacrawl_dev.en', 'w') as ed:\n",
    "    for train_j, train_e in zip(source_train, target_train):\n",
    "        print(train_j, file=jt)\n",
    "        print(train_e, file=et)\n",
    "    for dev_j, dev_e in zip(source_dev, target_dev):\n",
    "        print(dev_j,file=jd)\n",
    "        print(dev_e, file=ed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### japra事前学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-16 14:55:42 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='jparacrawl/jparacrawl-bin', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, simul_type=None, source_lang='ja', srcdict='jparacrawl/jparacrawl-bin/dict.ja.txt', suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer='space', tpu=False, trainpref='jparacrawl/jparacrawl_train', use_plasma_view=False, user_dir=None, validpref='jparacrawl/jparacrawl_dev', wandb_project=None, workers=20)\n",
      "2022-08-16 14:57:48 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 15336 types\n",
      "2022-08-16 15:07:22 | INFO | fairseq_cli.preprocess | [ja] jparacrawl/jparacrawl_train.ja: 18018044 sents, 698981737 tokens, 9.59e-05% replaced (by <unk>)\n",
      "2022-08-16 15:07:22 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 15336 types\n",
      "2022-08-16 15:11:28 | INFO | fairseq_cli.preprocess | [ja] jparacrawl/jparacrawl_dev.ja: 7722020 sents, 299565286 tokens, 0.000111% replaced (by <unk>)\n",
      "2022-08-16 15:11:28 | INFO | fairseq_cli.preprocess | [en] Dictionary: 4981944 types\n",
      "2022-08-16 15:17:38 | INFO | fairseq_cli.preprocess | [en] jparacrawl/jparacrawl_train.en: 18018044 sents, 437288578 tokens, 0.0% replaced (by <unk>)\n",
      "2022-08-16 15:17:38 | INFO | fairseq_cli.preprocess | [en] Dictionary: 4981944 types\n",
      "2022-08-16 15:21:08 | INFO | fairseq_cli.preprocess | [en] jparacrawl/jparacrawl_dev.en: 7722020 sents, 187438194 tokens, 0.713% replaced (by <unk>)\n",
      "2022-08-16 15:21:08 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to jparacrawl/jparacrawl-bin\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 fairseq-preprocess --source-lang ja --target-lang en \\\n",
    "   --trainpref jparacrawl/jparacrawl_train \\\n",
    "   --validpref jparacrawl/jparacrawl_dev \\\n",
    "   --tokenizer space \\\n",
    "   --destdir jparacrawl/jparacrawl-bin \\\n",
    "   --srcdict jparacrawl/jparacrawl-bin/dict.ja.txt \\\n",
    "   --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-07 13:18:53 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard/jparacrawl', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/jparacrawl', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='jparacrawl/jparacrawl-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=10, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_src_tgt_embed=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/jparacrawl', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir='tensorboard/jparacrawl', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'jparacrawl/jparacrawl-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-07 13:18:53 | INFO | fairseq.tasks.translation | [ja] dictionary: 15336 types\n",
      "2022-08-07 13:18:53 | INFO | fairseq.tasks.translation | [en] dictionary: 13824 types\n",
      "2022-08-07 13:18:54 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(15336, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(13824, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=13824, bias=False)\n",
      "  )\n",
      ")\n",
      "2022-08-07 13:18:54 | INFO | fairseq_cli.train | task: TranslationTask\n",
      "2022-08-07 13:18:54 | INFO | fairseq_cli.train | model: TransformerModel\n",
      "2022-08-07 13:18:54 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2022-08-07 13:18:54 | INFO | fairseq_cli.train | num. shared model params: 59,068,416 (num. trained: 59,068,416)\n",
      "2022-08-07 13:18:54 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2022-08-07 13:18:54 | INFO | fairseq.data.data_utils | loaded 7,722,020 examples from: jparacrawl/jparacrawl-bin/valid.ja-en.ja\n",
      "2022-08-07 13:18:54 | INFO | fairseq.data.data_utils | loaded 7,722,020 examples from: jparacrawl/jparacrawl-bin/valid.ja-en.en\n",
      "2022-08-07 13:18:54 | INFO | fairseq.tasks.translation | jparacrawl/jparacrawl-bin valid ja-en 7722020 examples\n",
      "2022-08-07 13:18:59 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2022-08-07 13:18:59 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-08-07 13:18:59 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 47.544 GB ; name = NVIDIA RTX A6000                        \n",
      "2022-08-07 13:18:59 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-08-07 13:18:59 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2022-08-07 13:18:59 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
      "2022-08-07 13:18:59 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/jparacrawl/checkpoint_last.pt\n",
      "2022-08-07 13:18:59 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/jparacrawl/checkpoint_last.pt\n",
      "2022-08-07 13:18:59 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2022-08-07 13:19:00 | INFO | fairseq.data.data_utils | loaded 18,018,044 examples from: jparacrawl/jparacrawl-bin/train.ja-en.ja\n",
      "2022-08-07 13:19:00 | INFO | fairseq.data.data_utils | loaded 18,018,044 examples from: jparacrawl/jparacrawl-bin/train.ja-en.en\n",
      "2022-08-07 13:19:00 | INFO | fairseq.tasks.translation | jparacrawl/jparacrawl-bin train ja-en 18018044 examples\n",
      "2022-08-07 13:19:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 187346\n",
      "epoch 001:   0%|                                     | 0/187346 [00:00<?, ?it/s]2022-08-07 13:19:06 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2022-08-07 13:19:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2022-08-07 13:19:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 001:   0%|                          | 1/187346 [00:00<49:57:19,  1.04it/s]2022-08-07 13:19:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "2022-08-07 13:19:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 001:   0%|                          | 3/187346 [00:01<17:14:31,  3.02it/s]2022-08-07 13:19:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 001:   0%|                          | 5/187346 [00:01<10:55:51,  4.76it/s]2022-08-07 13:19:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0\n",
      "epoch 001:   2%| | 3727/187346 [03:58<3:15:00, 15.69it/s, loss=3.554, nll_loss=22022-08-07 13:23:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0\n",
      "epoch 001:   2%| | 4291/187346 [04:34<3:13:11, 15.79it/s, loss=3.552, nll_loss=22022-08-07 13:23:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0\n",
      "epoch 001:   3%| | 5793/187346 [06:08<3:05:39, 16.30it/s, loss=3.534, nll_loss=2"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 fairseq-train jparacrawl/jparacrawl-bin  \\\n",
    "    --save-dir checkpoints/jparacrawl \\\n",
    "    --tensorboard-logdir tensorboard/jparacrawl \\\n",
    "    --arch transformer --task translation \\\n",
    "\t--share-decoder-input-output-embed \\\n",
    " \t--optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "\t--lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "\t--criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "\t--dropout 0.3 --weight-decay 0.0001 \\\n",
    "\t--max-tokens 4096 \\\n",
    "    --eval-bleu \\\n",
    "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
    "    --eval-bleu-detok space \\\n",
    "    --eval-bleu-remove-bpe \\\n",
    "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
    "    --max-epoch 10 \\\n",
    "\t--fp16 \\\n",
    "    --no-epoch-checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-18 06:32:56 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='kyoto/fine-tune-bin', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, simul_type=None, source_lang='ja', srcdict='jparacrawl/jparacrawl-bin/dict.ja.txt', suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref='kyoto/kyoto_sub-test', tgtdict='jparacrawl/jparacrawl-bin/dict.en.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer='space', tpu=False, trainpref='kyoto/kyoto_sub-train', use_plasma_view=False, user_dir=None, validpref='kyoto/kyoto_sub-dev', wandb_project=None, workers=20)\n",
      "2022-08-18 06:33:11 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 15336 types\n",
      "2022-08-18 06:33:24 | INFO | fairseq_cli.preprocess | [ja] kyoto/kyoto_sub-train.ja: 440288 sents, 10735573 tokens, 22.2% replaced (by <unk>)\n",
      "2022-08-18 06:33:24 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 15336 types\n",
      "2022-08-18 06:33:25 | INFO | fairseq_cli.preprocess | [ja] kyoto/kyoto_sub-dev.ja: 1166 sents, 24825 tokens, 24.0% replaced (by <unk>)\n",
      "2022-08-18 06:33:25 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 15336 types\n",
      "2022-08-18 06:33:25 | INFO | fairseq_cli.preprocess | [ja] kyoto/kyoto_sub-test.ja: 1160 sents, 26821 tokens, 23.2% replaced (by <unk>)\n",
      "2022-08-18 06:33:25 | INFO | fairseq_cli.preprocess | [en] Dictionary: 4981944 types\n",
      "2022-08-18 06:34:24 | INFO | fairseq_cli.preprocess | [en] kyoto/kyoto_sub-train.en: 440288 sents, 13280091 tokens, 25.1% replaced (by <unk>)\n",
      "2022-08-18 06:34:24 | INFO | fairseq_cli.preprocess | [en] Dictionary: 4981944 types\n",
      "2022-08-18 06:35:14 | INFO | fairseq_cli.preprocess | [en] kyoto/kyoto_sub-dev.en: 1166 sents, 29011 tokens, 27.5% replaced (by <unk>)\n",
      "2022-08-18 06:35:14 | INFO | fairseq_cli.preprocess | [en] Dictionary: 4981944 types\n",
      "2022-08-18 06:36:03 | INFO | fairseq_cli.preprocess | [en] kyoto/kyoto_sub-test.en: 1160 sents, 31468 tokens, 27.7% replaced (by <unk>)\n",
      "2022-08-18 06:36:03 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to kyoto/fine-tune-bin\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 fairseq-preprocess --source-lang ja --target-lang en \\\n",
    "   --trainpref kyoto/kyoto_sub-train \\\n",
    "   --validpref kyoto/kyoto_sub-dev \\\n",
    "   --testpref kyoto/kyoto_sub-test \\\n",
    "   --tokenizer space \\\n",
    "   --destdir kyoto/fine-tune-bin \\\n",
    "   --tgtdict jparacrawl/jparacrawl-bin/dict.en.txt \\\n",
    "   --srcdict jparacrawl/jparacrawl-bin/dict.ja.txt \\\n",
    "   --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-18 06:38:06 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard/kyoto_98/', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/kyoto_98/', 'restore_file': 'checkpoints/jparacrawl/checkpoint_best.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='kyoto/fine-tune-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.2, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0001], lr_scheduler='inverse_sqrt', max_epoch=10, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_src_tgt_embed=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoints/jparacrawl/checkpoint_best.pt', save_dir='checkpoints/kyoto_98/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir='tensorboard/kyoto_98/', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'kyoto/fine-tune-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2022-08-18 06:38:15 | INFO | fairseq.tasks.translation | [ja] dictionary: 15336 types\n",
      "2022-08-18 06:38:15 | INFO | fairseq.tasks.translation | [en] dictionary: 4981944 types\n",
      "2022-08-18 06:39:11 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(15336, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(4981944, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=4981944, bias=False)\n",
      "  )\n",
      ")\n",
      "2022-08-18 06:39:11 | INFO | fairseq_cli.train | task: TranslationTask\n",
      "2022-08-18 06:39:11 | INFO | fairseq_cli.train | model: TransformerModel\n",
      "2022-08-18 06:39:11 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2022-08-18 06:39:11 | INFO | fairseq_cli.train | num. shared model params: 2,602,745,856 (num. trained: 2,602,745,856)\n",
      "2022-08-18 06:39:11 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2022-08-18 06:39:11 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: kyoto/fine-tune-bin/valid.ja-en.ja\n",
      "2022-08-18 06:39:11 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: kyoto/fine-tune-bin/valid.ja-en.en\n",
      "2022-08-18 06:39:11 | INFO | fairseq.tasks.translation | kyoto/fine-tune-bin valid ja-en 1166 examples\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1 fairseq-train kyoto/fine-tune-bin  \\\n",
    "    --save-dir checkpoints/kyoto_98/ \\\n",
    "    --restore-file checkpoints/jparacrawl/checkpoint_best.pt \\\n",
    "    --tensorboard-logdir tensorboard/kyoto_98/ \\\n",
    "    --arch transformer --task translation \\\n",
    "    --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "    --lr 0.0001 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --dropout 0.2 --weight-decay 0.0001 \\\n",
    "    --max-tokens 4096 \\\n",
    "    --eval-bleu \\\n",
    "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
    "    --eval-bleu-detok space \\\n",
    "    --eval-bleu-remove-bpe \\\n",
    "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
    "    --fp16 \\\n",
    "    --max-epoch 10 \\\n",
    "    --no-epoch-checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 fairseq-interactive kyoto/fine-tune-bin \\\n",
    "    --path checkpoints/kyoto_98/checkpoint_best.pt  \\\n",
    "    --input kyoto/kyoto-test.ja \\\n",
    "    --task translation \\\n",
    "    | grep '^H' | cut -f3 > 98.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(ignore_case=False, order=4, ref='kyoto/kyoto-test.en', sacrebleu=False, sentence_bleu=False, sys='98.out')\n",
      "BLEU4 = 1.70, 16.1/3.0/0.7/0.3 (BP=1.000, ratio=1.007, syslen=26915, reflen=26734)\n"
     ]
    }
   ],
   "source": [
    "!fairseq-score --sys 98.out --ref kyoto/kyoto-test.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 99. 翻訳サーバの構築\n",
    "ユーザが翻訳したい文を入力すると，その翻訳結果がウェブブラウザ上で表示されるデモシステムを構築せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd 99\n",
    "# python3 translate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![result](./99/99result.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
